{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertForSequenceClassification: ['vocab_layer_norm', 'vocab_transform', 'activation_13', 'vocab_projector']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier', 'classifier', 'dropout_59']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "No loss specified in compile() - the model's internal loss computation will be used as the loss. Don't panic - this is a common way to train TensorFlow models in Transformers! To disable this behaviour, please pass a loss argument, or explicitly pass `loss=None` if you do not want your model to compute a loss.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "in user code:\n\n    File \"C:\\Users\\phili\\AppData\\Roaming\\Python\\Python38\\site-packages\\keras\\engine\\training.py\", line 1021, in train_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\phili\\AppData\\Roaming\\Python\\Python38\\site-packages\\keras\\engine\\training.py\", line 1010, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\phili\\AppData\\Roaming\\Python\\Python38\\site-packages\\keras\\engine\\training.py\", line 1000, in run_step  **\n        outputs = model.train_step(data)\n    File \"c:\\Users\\phili\\anaconda3\\envs\\smart-rejector\\lib\\site-packages\\transformers\\modeling_tf_utils.py\", line 996, in train_step\n        y_pred = self(x, training=True)\n    File \"C:\\Users\\phili\\AppData\\Roaming\\Python\\Python38\\site-packages\\keras\\utils\\traceback_utils.py\", line 67, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"C:\\Users\\phili\\AppData\\Local\\Temp\\__autograph_generated_file5zl1g702.py\", line 21, in tf__run_call_with_unpacked_inputs\n        retval_ = ag__.converted_call(ag__.ld(func), (ag__.ld(self),), dict(**ag__.ld(unpacked_inputs)), fscope)\n    File \"C:\\Users\\phili\\AppData\\Local\\Temp\\__autograph_generated_file1bzhdnl8.py\", line 11, in tf__call\n        distilbert_output = ag__.converted_call(ag__.ld(self).distilbert, (), dict(input_ids=ag__.ld(input_ids), attention_mask=ag__.ld(attention_mask), head_mask=ag__.ld(head_mask), inputs_embeds=ag__.ld(inputs_embeds), output_attentions=ag__.ld(output_attentions), output_hidden_states=ag__.ld(output_hidden_states), return_dict=ag__.ld(return_dict), training=ag__.ld(training)), fscope)\n    File \"C:\\Users\\phili\\AppData\\Local\\Temp\\__autograph_generated_file5zl1g702.py\", line 21, in tf__run_call_with_unpacked_inputs\n        retval_ = ag__.converted_call(ag__.ld(func), (ag__.ld(self),), dict(**ag__.ld(unpacked_inputs)), fscope)\n    File \"C:\\Users\\phili\\AppData\\Local\\Temp\\__autograph_generated_filev3rg17pn.py\", line 92, in tf__call\n        embedding_output = ag__.converted_call(ag__.ld(self).embeddings, (ag__.ld(input_ids),), dict(inputs_embeds=ag__.ld(inputs_embeds)), fscope)\n    File \"C:\\Users\\phili\\AppData\\Local\\Temp\\__autograph_generated_filewguyk7ec.py\", line 27, in tf__call\n        ag__.if_stmt((ag__.ld(input_ids) is not None), if_body, else_body, get_state, set_state, ('inputs_embeds',), 1)\n    File \"C:\\Users\\phili\\AppData\\Local\\Temp\\__autograph_generated_filewguyk7ec.py\", line 22, in if_body\n        inputs_embeds = ag__.converted_call(ag__.ld(tf).gather, (), dict(params=ag__.ld(self).weight, indices=ag__.ld(input_ids)), fscope)\n\n    TypeError: Exception encountered when calling layer \"tf_distil_bert_for_sequence_classification_2\" (type TFDistilBertForSequenceClassification).\n    \n    in user code:\n    \n        File \"c:\\Users\\phili\\anaconda3\\envs\\smart-rejector\\lib\\site-packages\\transformers\\modeling_tf_utils.py\", line 730, in run_call_with_unpacked_inputs  *\n            return func(self, **unpacked_inputs)\n        File \"c:\\Users\\phili\\anaconda3\\envs\\smart-rejector\\lib\\site-packages\\transformers\\models\\distilbert\\modeling_tf_distilbert.py\", line 742, in call  *\n            distilbert_output = self.distilbert(\n        File \"C:\\Users\\phili\\AppData\\Roaming\\Python\\Python38\\site-packages\\keras\\utils\\traceback_utils.py\", line 67, in error_handler  **\n            raise e.with_traceback(filtered_tb) from None\n        File \"C:\\Users\\phili\\AppData\\Local\\Temp\\__autograph_generated_file5zl1g702.py\", line 21, in tf__run_call_with_unpacked_inputs\n            retval_ = ag__.converted_call(ag__.ld(func), (ag__.ld(self),), dict(**ag__.ld(unpacked_inputs)), fscope)\n        File \"C:\\Users\\phili\\AppData\\Local\\Temp\\__autograph_generated_filev3rg17pn.py\", line 92, in tf__call\n            embedding_output = ag__.converted_call(ag__.ld(self).embeddings, (ag__.ld(input_ids),), dict(inputs_embeds=ag__.ld(inputs_embeds)), fscope)\n        File \"C:\\Users\\phili\\AppData\\Local\\Temp\\__autograph_generated_filewguyk7ec.py\", line 27, in tf__call\n            ag__.if_stmt((ag__.ld(input_ids) is not None), if_body, else_body, get_state, set_state, ('inputs_embeds',), 1)\n        File \"C:\\Users\\phili\\AppData\\Local\\Temp\\__autograph_generated_filewguyk7ec.py\", line 22, in if_body\n            inputs_embeds = ag__.converted_call(ag__.ld(tf).gather, (), dict(params=ag__.ld(self).weight, indices=ag__.ld(input_ids)), fscope)\n    \n        TypeError: Exception encountered when calling layer \"distilbert\" (type TFDistilBertMainLayer).\n        \n        in user code:\n        \n            File \"c:\\Users\\phili\\anaconda3\\envs\\smart-rejector\\lib\\site-packages\\transformers\\modeling_tf_utils.py\", line 730, in run_call_with_unpacked_inputs  *\n                return func(self, **unpacked_inputs)\n            File \"c:\\Users\\phili\\anaconda3\\envs\\smart-rejector\\lib\\site-packages\\transformers\\models\\distilbert\\modeling_tf_distilbert.py\", line 400, in call  *\n                embedding_output = self.embeddings(input_ids, inputs_embeds=inputs_embeds)  # (bs, seq_length, dim)\n            File \"C:\\Users\\phili\\AppData\\Roaming\\Python\\Python38\\site-packages\\keras\\utils\\traceback_utils.py\", line 67, in error_handler  **\n                raise e.with_traceback(filtered_tb) from None\n            File \"C:\\Users\\phili\\AppData\\Local\\Temp\\__autograph_generated_filewguyk7ec.py\", line 27, in tf__call\n                ag__.if_stmt((ag__.ld(input_ids) is not None), if_body, else_body, get_state, set_state, ('inputs_embeds',), 1)\n            File \"C:\\Users\\phili\\AppData\\Local\\Temp\\__autograph_generated_filewguyk7ec.py\", line 22, in if_body\n                inputs_embeds = ag__.converted_call(ag__.ld(tf).gather, (), dict(params=ag__.ld(self).weight, indices=ag__.ld(input_ids)), fscope)\n        \n            TypeError: Exception encountered when calling layer \"embeddings\" (type TFEmbeddings).\n            \n            in user code:\n            \n                File \"c:\\Users\\phili\\anaconda3\\envs\\smart-rejector\\lib\\site-packages\\transformers\\models\\distilbert\\modeling_tf_distilbert.py\", line 113, in call  *\n                    inputs_embeds = tf.gather(params=self.weight, indices=input_ids)\n            \n                TypeError: Value passed to parameter 'indices' has DataType string not in list of allowed values: int32, int64\n            \n            \n            Call arguments received by layer \"embeddings\" (type TFEmbeddings):\n              • input_ids=tf.Tensor(shape=(), dtype=string)\n              • position_ids=None\n              • inputs_embeds=None\n              • training=True\n        \n        \n        Call arguments received by layer \"distilbert\" (type TFDistilBertMainLayer):\n          • self=tf.Tensor(shape=(), dtype=string)\n          • input_ids=None\n          • attention_mask=None\n          • head_mask=None\n          • inputs_embeds=None\n          • output_attentions=False\n          • output_hidden_states=False\n          • return_dict=True\n          • training=True\n    \n    \n    Call arguments received by layer \"tf_distil_bert_for_sequence_classification_2\" (type TFDistilBertForSequenceClassification):\n      • self={'input_ids': 'tf.Tensor(shape=(), dtype=string)', 'labels': 'tf.Tensor(shape=(), dtype=string)'}\n      • input_ids=None\n      • attention_mask=None\n      • head_mask=None\n      • inputs_embeds=None\n      • output_attentions=None\n      • output_hidden_states=None\n      • return_dict=None\n      • labels=None\n      • training=True\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mf:\\Development\\smart_rejector_for_hate_speech\\code\\model\\Train DistilBERT.ipynb Cell 1'\u001b[0m in \u001b[0;36m<cell line: 27>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/Development/smart_rejector_for_hate_speech/code/model/Train%20DistilBERT.ipynb#ch0000000?line=24'>25</a>\u001b[0m model \u001b[39m=\u001b[39m TFAutoModelForSequenceClassification\u001b[39m.\u001b[39mfrom_pretrained(\u001b[39m\"\u001b[39m\u001b[39mdistilbert-base-uncased\u001b[39m\u001b[39m\"\u001b[39m, num_labels\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/Development/smart_rejector_for_hate_speech/code/model/Train%20DistilBERT.ipynb#ch0000000?line=25'>26</a>\u001b[0m model\u001b[39m.\u001b[39mcompile(optimizer\u001b[39m=\u001b[39moptimizer)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/f%3A/Development/smart_rejector_for_hate_speech/code/model/Train%20DistilBERT.ipynb#ch0000000?line=26'>27</a>\u001b[0m model\u001b[39m.\u001b[39;49mfit(x\u001b[39m=\u001b[39;49mtf_train, validation_data\u001b[39m=\u001b[39;49mtf_test, epochs\u001b[39m=\u001b[39;49m\u001b[39m3\u001b[39;49m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\keras\\utils\\traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     66\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m---> 67\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m     68\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     69\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_filek9ghres8.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     retval_ \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(step_function), (ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m), ag__\u001b[39m.\u001b[39mld(iterator)), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[0;32m     16\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\phili\\anaconda3\\envs\\smart-rejector\\lib\\site-packages\\transformers\\modeling_tf_utils.py:996\u001b[0m, in \u001b[0;36mTFPreTrainedModel.train_step\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m    994\u001b[0m \u001b[39m# Run forward pass.\u001b[39;00m\n\u001b[0;32m    995\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mGradientTape() \u001b[39mas\u001b[39;00m tape:\n\u001b[1;32m--> 996\u001b[0m     y_pred \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m(x, training\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m    997\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_using_dummy_loss:\n\u001b[0;32m    998\u001b[0m         loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompiled_loss(y_pred\u001b[39m.\u001b[39mloss, y_pred\u001b[39m.\u001b[39mloss, sample_weight, regularization_losses\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlosses)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_file5zl1g702.py:21\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__run_call_with_unpacked_inputs\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     20\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m     retval_ \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(func), (ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m),), \u001b[39mdict\u001b[39m(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mag__\u001b[39m.\u001b[39mld(unpacked_inputs)), fscope)\n\u001b[0;32m     22\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[0;32m     23\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_file1bzhdnl8.py:11\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__call\u001b[1;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict, labels, training)\u001b[0m\n\u001b[0;32m      9\u001b[0m do_return \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m     10\u001b[0m retval_ \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mUndefinedReturnValue()\n\u001b[1;32m---> 11\u001b[0m distilbert_output \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39mdistilbert, (), \u001b[39mdict\u001b[39m(input_ids\u001b[39m=\u001b[39mag__\u001b[39m.\u001b[39mld(input_ids), attention_mask\u001b[39m=\u001b[39mag__\u001b[39m.\u001b[39mld(attention_mask), head_mask\u001b[39m=\u001b[39mag__\u001b[39m.\u001b[39mld(head_mask), inputs_embeds\u001b[39m=\u001b[39mag__\u001b[39m.\u001b[39mld(inputs_embeds), output_attentions\u001b[39m=\u001b[39mag__\u001b[39m.\u001b[39mld(output_attentions), output_hidden_states\u001b[39m=\u001b[39mag__\u001b[39m.\u001b[39mld(output_hidden_states), return_dict\u001b[39m=\u001b[39mag__\u001b[39m.\u001b[39mld(return_dict), training\u001b[39m=\u001b[39mag__\u001b[39m.\u001b[39mld(training)), fscope)\n\u001b[0;32m     12\u001b[0m hidden_state \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mld(distilbert_output)[\u001b[39m0\u001b[39m]\n\u001b[0;32m     13\u001b[0m pooled_output \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mld(hidden_state)[:, \u001b[39m0\u001b[39m]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_file5zl1g702.py:21\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__run_call_with_unpacked_inputs\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     20\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m     retval_ \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(func), (ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m),), \u001b[39mdict\u001b[39m(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mag__\u001b[39m.\u001b[39mld(unpacked_inputs)), fscope)\n\u001b[0;32m     22\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[0;32m     23\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_filev3rg17pn.py:92\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__call\u001b[1;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict, training)\u001b[0m\n\u001b[0;32m     90\u001b[0m     head_mask \u001b[39m=\u001b[39m ([\u001b[39mNone\u001b[39;00m] \u001b[39m*\u001b[39m ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39mnum_hidden_layers)\n\u001b[0;32m     91\u001b[0m ag__\u001b[39m.\u001b[39mif_stmt((ag__\u001b[39m.\u001b[39mld(head_mask) \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m), if_body_4, else_body_4, get_state_4, set_state_4, (\u001b[39m'\u001b[39m\u001b[39mhead_mask\u001b[39m\u001b[39m'\u001b[39m,), \u001b[39m1\u001b[39m)\n\u001b[1;32m---> 92\u001b[0m embedding_output \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39membeddings, (ag__\u001b[39m.\u001b[39mld(input_ids),), \u001b[39mdict\u001b[39m(inputs_embeds\u001b[39m=\u001b[39mag__\u001b[39m.\u001b[39mld(inputs_embeds)), fscope)\n\u001b[0;32m     93\u001b[0m tfmr_output \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39mtransformer, (ag__\u001b[39m.\u001b[39mld(embedding_output), ag__\u001b[39m.\u001b[39mld(attention_mask), ag__\u001b[39m.\u001b[39mld(head_mask), ag__\u001b[39m.\u001b[39mld(output_attentions), ag__\u001b[39m.\u001b[39mld(output_hidden_states), ag__\u001b[39m.\u001b[39mld(return_dict)), \u001b[39mdict\u001b[39m(training\u001b[39m=\u001b[39mag__\u001b[39m.\u001b[39mld(training)), fscope)\n\u001b[0;32m     94\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_filewguyk7ec.py:27\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__call\u001b[1;34m(self, input_ids, position_ids, inputs_embeds, training)\u001b[0m\n\u001b[0;32m     25\u001b[0m     \u001b[39mnonlocal\u001b[39;00m inputs_embeds\n\u001b[0;32m     26\u001b[0m     \u001b[39mpass\u001b[39;00m\n\u001b[1;32m---> 27\u001b[0m ag__\u001b[39m.\u001b[39mif_stmt((ag__\u001b[39m.\u001b[39mld(input_ids) \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m), if_body, else_body, get_state, set_state, (\u001b[39m'\u001b[39m\u001b[39minputs_embeds\u001b[39m\u001b[39m'\u001b[39m,), \u001b[39m1\u001b[39m)\n\u001b[0;32m     28\u001b[0m input_shape \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(shape_list), (ag__\u001b[39m.\u001b[39mld(inputs_embeds),), \u001b[39mNone\u001b[39;00m, fscope)[:(\u001b[39m-\u001b[39m \u001b[39m1\u001b[39m)]\n\u001b[0;32m     30\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_state_1\u001b[39m():\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_filewguyk7ec.py:22\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__call.<locals>.if_body\u001b[1;34m()\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mif_body\u001b[39m():\n\u001b[0;32m     21\u001b[0m     \u001b[39mnonlocal\u001b[39;00m inputs_embeds\n\u001b[1;32m---> 22\u001b[0m     inputs_embeds \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39;49mconverted_call(ag__\u001b[39m.\u001b[39;49mld(tf)\u001b[39m.\u001b[39;49mgather, (), \u001b[39mdict\u001b[39;49m(params\u001b[39m=\u001b[39;49mag__\u001b[39m.\u001b[39;49mld(\u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49mweight, indices\u001b[39m=\u001b[39;49mag__\u001b[39m.\u001b[39;49mld(input_ids)), fscope)\n",
      "\u001b[1;31mTypeError\u001b[0m: in user code:\n\n    File \"C:\\Users\\phili\\AppData\\Roaming\\Python\\Python38\\site-packages\\keras\\engine\\training.py\", line 1021, in train_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\phili\\AppData\\Roaming\\Python\\Python38\\site-packages\\keras\\engine\\training.py\", line 1010, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\phili\\AppData\\Roaming\\Python\\Python38\\site-packages\\keras\\engine\\training.py\", line 1000, in run_step  **\n        outputs = model.train_step(data)\n    File \"c:\\Users\\phili\\anaconda3\\envs\\smart-rejector\\lib\\site-packages\\transformers\\modeling_tf_utils.py\", line 996, in train_step\n        y_pred = self(x, training=True)\n    File \"C:\\Users\\phili\\AppData\\Roaming\\Python\\Python38\\site-packages\\keras\\utils\\traceback_utils.py\", line 67, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"C:\\Users\\phili\\AppData\\Local\\Temp\\__autograph_generated_file5zl1g702.py\", line 21, in tf__run_call_with_unpacked_inputs\n        retval_ = ag__.converted_call(ag__.ld(func), (ag__.ld(self),), dict(**ag__.ld(unpacked_inputs)), fscope)\n    File \"C:\\Users\\phili\\AppData\\Local\\Temp\\__autograph_generated_file1bzhdnl8.py\", line 11, in tf__call\n        distilbert_output = ag__.converted_call(ag__.ld(self).distilbert, (), dict(input_ids=ag__.ld(input_ids), attention_mask=ag__.ld(attention_mask), head_mask=ag__.ld(head_mask), inputs_embeds=ag__.ld(inputs_embeds), output_attentions=ag__.ld(output_attentions), output_hidden_states=ag__.ld(output_hidden_states), return_dict=ag__.ld(return_dict), training=ag__.ld(training)), fscope)\n    File \"C:\\Users\\phili\\AppData\\Local\\Temp\\__autograph_generated_file5zl1g702.py\", line 21, in tf__run_call_with_unpacked_inputs\n        retval_ = ag__.converted_call(ag__.ld(func), (ag__.ld(self),), dict(**ag__.ld(unpacked_inputs)), fscope)\n    File \"C:\\Users\\phili\\AppData\\Local\\Temp\\__autograph_generated_filev3rg17pn.py\", line 92, in tf__call\n        embedding_output = ag__.converted_call(ag__.ld(self).embeddings, (ag__.ld(input_ids),), dict(inputs_embeds=ag__.ld(inputs_embeds)), fscope)\n    File \"C:\\Users\\phili\\AppData\\Local\\Temp\\__autograph_generated_filewguyk7ec.py\", line 27, in tf__call\n        ag__.if_stmt((ag__.ld(input_ids) is not None), if_body, else_body, get_state, set_state, ('inputs_embeds',), 1)\n    File \"C:\\Users\\phili\\AppData\\Local\\Temp\\__autograph_generated_filewguyk7ec.py\", line 22, in if_body\n        inputs_embeds = ag__.converted_call(ag__.ld(tf).gather, (), dict(params=ag__.ld(self).weight, indices=ag__.ld(input_ids)), fscope)\n\n    TypeError: Exception encountered when calling layer \"tf_distil_bert_for_sequence_classification_2\" (type TFDistilBertForSequenceClassification).\n    \n    in user code:\n    \n        File \"c:\\Users\\phili\\anaconda3\\envs\\smart-rejector\\lib\\site-packages\\transformers\\modeling_tf_utils.py\", line 730, in run_call_with_unpacked_inputs  *\n            return func(self, **unpacked_inputs)\n        File \"c:\\Users\\phili\\anaconda3\\envs\\smart-rejector\\lib\\site-packages\\transformers\\models\\distilbert\\modeling_tf_distilbert.py\", line 742, in call  *\n            distilbert_output = self.distilbert(\n        File \"C:\\Users\\phili\\AppData\\Roaming\\Python\\Python38\\site-packages\\keras\\utils\\traceback_utils.py\", line 67, in error_handler  **\n            raise e.with_traceback(filtered_tb) from None\n        File \"C:\\Users\\phili\\AppData\\Local\\Temp\\__autograph_generated_file5zl1g702.py\", line 21, in tf__run_call_with_unpacked_inputs\n            retval_ = ag__.converted_call(ag__.ld(func), (ag__.ld(self),), dict(**ag__.ld(unpacked_inputs)), fscope)\n        File \"C:\\Users\\phili\\AppData\\Local\\Temp\\__autograph_generated_filev3rg17pn.py\", line 92, in tf__call\n            embedding_output = ag__.converted_call(ag__.ld(self).embeddings, (ag__.ld(input_ids),), dict(inputs_embeds=ag__.ld(inputs_embeds)), fscope)\n        File \"C:\\Users\\phili\\AppData\\Local\\Temp\\__autograph_generated_filewguyk7ec.py\", line 27, in tf__call\n            ag__.if_stmt((ag__.ld(input_ids) is not None), if_body, else_body, get_state, set_state, ('inputs_embeds',), 1)\n        File \"C:\\Users\\phili\\AppData\\Local\\Temp\\__autograph_generated_filewguyk7ec.py\", line 22, in if_body\n            inputs_embeds = ag__.converted_call(ag__.ld(tf).gather, (), dict(params=ag__.ld(self).weight, indices=ag__.ld(input_ids)), fscope)\n    \n        TypeError: Exception encountered when calling layer \"distilbert\" (type TFDistilBertMainLayer).\n        \n        in user code:\n        \n            File \"c:\\Users\\phili\\anaconda3\\envs\\smart-rejector\\lib\\site-packages\\transformers\\modeling_tf_utils.py\", line 730, in run_call_with_unpacked_inputs  *\n                return func(self, **unpacked_inputs)\n            File \"c:\\Users\\phili\\anaconda3\\envs\\smart-rejector\\lib\\site-packages\\transformers\\models\\distilbert\\modeling_tf_distilbert.py\", line 400, in call  *\n                embedding_output = self.embeddings(input_ids, inputs_embeds=inputs_embeds)  # (bs, seq_length, dim)\n            File \"C:\\Users\\phili\\AppData\\Roaming\\Python\\Python38\\site-packages\\keras\\utils\\traceback_utils.py\", line 67, in error_handler  **\n                raise e.with_traceback(filtered_tb) from None\n            File \"C:\\Users\\phili\\AppData\\Local\\Temp\\__autograph_generated_filewguyk7ec.py\", line 27, in tf__call\n                ag__.if_stmt((ag__.ld(input_ids) is not None), if_body, else_body, get_state, set_state, ('inputs_embeds',), 1)\n            File \"C:\\Users\\phili\\AppData\\Local\\Temp\\__autograph_generated_filewguyk7ec.py\", line 22, in if_body\n                inputs_embeds = ag__.converted_call(ag__.ld(tf).gather, (), dict(params=ag__.ld(self).weight, indices=ag__.ld(input_ids)), fscope)\n        \n            TypeError: Exception encountered when calling layer \"embeddings\" (type TFEmbeddings).\n            \n            in user code:\n            \n                File \"c:\\Users\\phili\\anaconda3\\envs\\smart-rejector\\lib\\site-packages\\transformers\\models\\distilbert\\modeling_tf_distilbert.py\", line 113, in call  *\n                    inputs_embeds = tf.gather(params=self.weight, indices=input_ids)\n            \n                TypeError: Value passed to parameter 'indices' has DataType string not in list of allowed values: int32, int64\n            \n            \n            Call arguments received by layer \"embeddings\" (type TFEmbeddings):\n              • input_ids=tf.Tensor(shape=(), dtype=string)\n              • position_ids=None\n              • inputs_embeds=None\n              • training=True\n        \n        \n        Call arguments received by layer \"distilbert\" (type TFDistilBertMainLayer):\n          • self=tf.Tensor(shape=(), dtype=string)\n          • input_ids=None\n          • attention_mask=None\n          • head_mask=None\n          • inputs_embeds=None\n          • output_attentions=False\n          • output_hidden_states=False\n          • return_dict=True\n          • training=True\n    \n    \n    Call arguments received by layer \"tf_distil_bert_for_sequence_classification_2\" (type TFDistilBertForSequenceClassification):\n      • self={'input_ids': 'tf.Tensor(shape=(), dtype=string)', 'labels': 'tf.Tensor(shape=(), dtype=string)'}\n      • input_ids=None\n      • attention_mask=None\n      • head_mask=None\n      • inputs_embeds=None\n      • output_attentions=None\n      • output_hidden_states=None\n      • return_dict=None\n      • labels=None\n      • training=True\n"
     ]
    }
   ],
   "source": [
    "from transformers import DistilBertTokenizer, TFAutoModelForSequenceClassification\n",
    "from reader import Reader\n",
    "import tensorflow as tf\n",
    "from transformers import create_optimizer\n",
    "# https://huggingface.co/docs/transformers/tasks/sequence_classification\n",
    "NUM_CLASSES = 2\n",
    "LEARN_RATE = 0.01\n",
    "EMBED_SIZE = 50\n",
    "FILENAME = \"data/twitter_data.pkl\"\n",
    "\n",
    "reader = Reader(filename=FILENAME, num_classes=NUM_CLASSES)\n",
    "X, y = reader.load()\n",
    "X_train, X_test, y_train, y_test = reader.split(X, y)\n",
    "\n",
    "tf_train = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "tf_test = tf.data.Dataset.from_tensor_slices((X_test, y_test))\n",
    "\n",
    "batch_size = 16\n",
    "num_epochs = 5\n",
    "batches_per_epoch = len(X_train) // batch_size\n",
    "total_train_steps = int(batches_per_epoch * num_epochs)\n",
    "optimizer, schedule = create_optimizer(init_lr=2e-5, num_warmup_steps=0, num_train_steps=total_train_steps)\n",
    "\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=2)\n",
    "model.compile(optimizer=optimizer)\n",
    "model.fit(x=tf_train, validation_data=tf_test, epochs=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('smart-rejector')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1d6d5fb1668883c33dcb7e6d97ab558619c066a76f945629b031854efe98e76a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
