\chapter{Introduction}
\newcommand{\customtextbox}[1]{
	\setlength{\fboxsep}{0.5em}
	\fbox{
		\begin{minipage}{\linewidth-1.7em}
			\vspace*{0.25em}
			#1
		\end{minipage}
	}
}

The amount of hateful content spread online on social media platforms remains a significant problem. Ignoring its presence can harm people and even result in actual violence and other conflicts \cite{ecri-hate-speech-and-violence, balayn2021automatic}. There are many news articles about events where hate spread on online platforms lead to acts of violence \cite{columbia-facebook-linked-to-violence, mujib-mashal-india, paul-mozur-2018, muller2021fanning}. One research paper found a connection between hateful content on Facebook containing anti-refugee sentiment and hate crimes against refugees by analyzing social media usage in multiple municipalities in Germany  \cite{muller2021fanning}. Governmental institutions and social media companies are becoming more aware of these risks and are trying to tackle hate speech. For example, the European Union developed a Code of Conduct on countering illegal hate speech in cooperation with large social media companies such as Facebook and Twitter \cite{eu-code-of-conduct}. This Code of Conduct requests companies to prohibit hate speech and report their progress every year \cite{eu-code-of-conduct}. The most recent report from 2021 stated that Facebook is most successful in removing hate speech as they claim that they removed 70.2\% of all hateful content in 2021 \cite{eu-code-of-conduct}. However, one article found in internal communication from Facebook that this percentage is much lower, around 3-5\% \cite{noah2021giansiracusa}. Twitter

Manual moderation of hateful content is still the most reliable solution but simply infeasible due to the large amount of content generated by the many users \cite{balayn2021automatic}. Therefore, Facebook, for example, adopts both reactive and proactive content moderation \cite{klonick2017new}. Reactive manual moderation is when users are flagging (also known as reporting) hateful content \cite{klonick2017new}. The main benefit of this is that many users can check a large amount of content. However, the problem remains that the users are still seeing hateful content for some time. Proactive moderation is either done automatically using detection algorithms or by a dedicated group of employees from Facebook \cite{klonick2017new}. However, it remains unclear how companies such as Facebook are automatically moderating content since their exact practices are not publicly available \cite{klonick2017new}. 

There exist different methods for automatically detecting hateful content when looking at current practices from literature. Most use Machine Learning (ML) algorithms since these tend to be the most promising for their detection performance at a large scale \cite{balayn2021automatic, fortuna2018survey}. These algorithms can range from traditional ML methods such as Support Vector Machine or Decision Tree to Deep Learning algorithms \cite{fortuna2018survey}.

However, these algorithms can be unreliable as they often perform poor on deployment data \cite{balayn2021automatic}. For instance, one paper found that most research in hate speech detection overestimates the performance of the automated detection methods \cite{arango2019hate}. The authors found that there is a significant performance drop when the detection algorithms are trained on one dataset and evaluated on another \cite{arango2019hate}. Furthermore, internal communication at Facebook indicates that ML algorithms are still not effective enough \cite{noah2021giansiracusa}.

Therefore, there is a need for a \textit{socio-technical} or \textit{human-machine co-creation} \cite{woo2020future} system that combines the advantages of both humans (cognitive abilities and ability to make judgements) and machines (automation and performance). A system where humans and machines work together to detect hate speech more effectively. This system should be a \textit{machine-assisting-human} system where ML models are helping humans to detect hateful content automatically and where humans can make the final decisions (\textit{human-in-the-loop}) when the model is not confident enough \cite{woo2020future}. This leads to our main research question:

\customtextbox{\textbf{RQ} How can we use human computation in detecting hate speech?}

Here come ML models with a reject option in place. The goal of the reject option is to reject an ML prediction when the risk of making an incorrect prediction is too high and to defer the prediction task to a human \cite{hendrickx2021machine}. There are several advantages. First, the utility of the ML increases as only the most confident (and possibly the most correct) predictions are accepted. Second, less human effort is necessary as the machine is handling all predictions tasks, and only a fraction needs to be checked by a human. So far, ML with rejection has not been used in hate speech detection yet. Therefore, the goal of this thesis project is to build the first \textit{smart rejector for detecting hate speech}. 

But how can we determine whether to reject or accept a prediction? We can argue that there are gains in making correct predictions and costs of rejection and making incorrect predictions. More specifically, we should weigh cost values for False Negative (FN), labelling something as non-hateful when it is, and False Positive (FP) predictions, labelling something as hateful when it is not, according to the task \cite{sayin2021science}. So, we need a metric that measures the cost-effectiveness of the smart rejector. We can use the resulting metric to determine when to reject/accept predictions by maximizing cost-effectiveness. Therefore, our first sub research question is as follows:

\customtextbox{\textbf{SQ1} How can we measure the cost-effectiveness of the reject option?}

The idea of ML models with rejection is that predictions are rejected when the confidence of the prediction is too low. However, there also exist cases for which the ML model produces high confidence but incorrect predictions. These high confident errors are also called \textit{unknown unknowns} \cite{liu2020towards}. We can further improve the smart rejector by detecting these unknown unknowns. This leads to our second sub research question:

\customtextbox{\textbf{SQ2} How can we find the unknown unknowns?}

Finally, we need to find out how we can combine our findings into one smart rejection system which leads to our final sub research question:

\customtextbox{\textbf{SQ3} How can we build the smart rejector?}

\todo[inline]{Here comes a list of contributions}

\todo[inline]{Here comes a short description of the structure of the thesis report}
