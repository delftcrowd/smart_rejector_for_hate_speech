\chapter{Introduction}
\newcommand{\customtextbox}[1]{
	\setlength{\fboxsep}{0.5em}
	\fbox{
		\begin{minipage}{\linewidth-1.7em}
			\vspace*{0.25em}
			#1
		\end{minipage}
	}
}

The amount of hateful content spread online on social media platforms remains a significant problem. Ignoring its presence can harm people and even result in actual violence and other conflicts \cite{ecri-hate-speech-and-violence, balayn2021automatic}. There are many news articles about events where hate spread on online platforms lead to acts of violence \cite{columbia-facebook-linked-to-violence, mujib-mashal-india, paul-mozur-2018, muller2021fanning}. One research paper found a connection between hateful content on Facebook containing anti-refugee sentiment and hate crimes against refugees by analyzing social media usage in multiple municipalities in Germany  \cite{muller2021fanning}. Governmental institutions and social media companies are becoming more aware of these risks and are trying to tackle hate speech. For example, the European Union developed a Code of Conduct on countering illegal hate speech in cooperation with large social media companies such as Facebook and Twitter \cite{eu-code-of-conduct}. This Code of Conduct requests companies to prohibit hate speech and report their progress every year \cite{eu-code-of-conduct}. The most recent report from 2021 stated that Twitter only removed 49.5\% of all hateful content on their platform. Facebook is most successful in removing hate speech as they claim to have removed 70.2\% of all hateful content in 2021 \cite{eu-code-of-conduct}. However, one article found in internal communication from Facebook that this percentage is much lower, around 3-5\% \cite{noah2021giansiracusa}. Therefore, hate speech detection remains a hard problem that even large institutions cannot do well yet.

Currently, people rely on reactive and proactive content moderation methods to detect hate speech \cite{klonick2017new}. Reactive moderation is when social media users are flagging (also known as reporting) hateful content \cite{klonick2017new}. Proactive moderation is either done automatically using detection algorithms or manually by a group of human moderators \cite{klonick2017new}. There exist different methods for automatically detecting hateful content. Most use Machine Learning (ML) algorithms since these tend to be the most promising for their detection performance at a large scale \cite{balayn2021automatic, fortuna2018survey}. These algorithms can range from traditional ML methods such as Support Vector Machine or Decision Tree to Deep Learning algorithms \cite{fortuna2018survey}.

However, both proactive and reactive moderation methods have their limitations. Proactive manual moderation of hateful content is still the most reliable solution but is simply infeasible due to the large amount of content generated by the many users \cite{balayn2021automatic}. Reactive moderation solves this problem since the users can report hate speech themselves. Although, the problem stays that hateful content is exposed to the users for some time. Proactive automatic moderation using automated detection algorithms allow for large amounts of data to be checked quickly without the involvement of humans. However, these algorithms can be unreliable as they often perform poor on deployment data \cite{balayn2021automatic}. Furthermore, one paper found that most research in hate speech detection overestimates the performance of the automated detection methods \cite{arango2019hate}. The authors found that the performance drops significantly when the detection algorithms are trained on one dataset and evaluated on another \cite{arango2019hate}.

This thesis research will tackle the problems of proactive moderation by creating a \textit{human-machine co-creation} \cite{woo2020future} system that combines the advantages of both humans (cognitive abilities and ability to make judgements) and machines (automation and performance). A system where humans and machines work together to detect hate speech more effectively. This system is a \textit{machine-assisting-human} system where ML models help humans detecting hateful content automatically and where humans can make the final decisions (\textit{human-in-the-loop}) when the model is not confident enough \cite{woo2020future}. Here come ML models with a reject option in place. The goal of the reject option is to reject an ML prediction when the risk of making an incorrect prediction is too high and to defer the prediction task to a human \cite{hendrickx2021machine}. There are several advantages. First, the utility of the ML increases as only the most confident (and possibly the most correct) predictions are accepted. Second, less human effort is necessary as the machine is handling all predictions tasks, and only a fraction needs to be checked by a human. To the best of our knowledge, ML with rejection has not been used in hate speech detection before. Therefore, the goal of this thesis project is to build the first \textit{smart rejector for detecting hate speech}. This leads to our main research question:

\customtextbox{\textbf{RQ} How can we apply Machine Learning with a reject option to detect hate speech more effectively?}

% How can we detect hate speech more effectively by combining both manual and automatic detection methods?
% How can we effectively reject hate speech predictions?
% How can we maximize the utility of automated hate speechd detection methods using Machine Learning with a reject option?
% How can we improve automated hate speech detection methods using ?
% How can we apply Machine Learning with a reject option to detect hate speech more effectively/reliably ?

The idea of most ML models with rejection is that we reject predictions when the model's confidence is too low. However, we need to tackle two underlying problems to be able to answer our main research question. First, we need to figure out when the ML model is not confident enough by measuring the effectiveness of the reject option according to the task of hate speech detection. Second, we need to determine how we can detect the high confidence errors.

The first problem is about the trade-off between how much we trust the predictions produced by the ML model and how much we involve humans to make the judgements. There are gains of accepting correct predictions, costs of accepting incorrect predictions, and costs of rejecting samples. More specifically, we should weigh cost values for False Negative (FN), labelling something as non-hateful when it is, and False Positive (FP) predictions, labelling something as hateful when it is not, according to the task \cite{sayin2021science}. So, we first need a metric that measures the cost-effectiveness of the reject option. We can use the resulting metric to determine when to reject/accept predictions by maximizing cost-effectiveness. Second, we need to find out how we can define these cost values in the context of hate speech detection. Therefore, our first sub research question is as follows:

\customtextbox{
	\textbf{SRQ1} How can we determine when the Machine Learning model is not confident enough?
	\begin{itemize}
		\item \textbf{SRQ1.1} How can we measure the cost-effectiveness of the reject option?
		\item \textbf{SRQ1.2} How can we determine the costs of rejections and True Positive (TP), True Negative (TN), False Positive (FP), and False Negative (FN) predictions?
	\end{itemize}
}

The second problem is about detecting the high confidence errors. These high confidence errors are also called \textit{unknown unknowns} \cite{liu2020towards}. When we would only rely on the confidence of the ML model to determine when to reject/accept predictions, then we would accept a subset of incorrect predictions with high confidence. We need to find a way to recognize these unknown unknowns. Once detected, we can reject these samples so that a human moderator makes the final judgement. Doing so would further improve the effectiveness of our smart rejector for detecting hate speech. So, our second sub research question is as follows:

\customtextbox{\textbf{SRQ2} How can we detect the unknown unknowns?}

% Finally, we need to find out how we can combine our findings into one smart rejection system which leads to our final sub research question:

% \customtextbox{\textbf{SRQ3} How can we build the smart rejector?}

\todo[inline]{Here comes a list of contributions}

\todo[inline]{Here comes a short description of the structure of the thesis report}
