\chapter{Related work}
This section briefly defines hate speech and why it is such a challenging topic to tackle, especially in computer science.
%
Then, we give an overview of some algorithms we can use to detect hate speech automatically.
%
We provide examples of rejecting ML model predictions and discuss the main challenges of assessing the values of (in)correct and rejected predictions in the context of hate speech detection.
%
Finally, we discuss why standard metric, such as accuracy, are unsuitable to evaluate hate speech detection models and why we need human-centred metrics instead.

\section{Challenges of hate speech detection}
Different types of online conflictual languages exist, such as cyberbullying, offensive language, toxic language, or hate speech, and come with varying definitions from domains such as psychology, political science, or computer science \citep{balayn2021automatic}.
%
We can broadly define \textit{hate speech} as ``language that is used to express hatred towards a targeted group or is intended to be derogatory, to humiliate, or to insult the members of the group'' \citep{davidson2017automated, balayn2021automatic}.
%
It differs from other conflictual languages since it focuses on specific target groups or individuals \citep{balayn2021automatic}.
%
However, many more definitions exist in literature mainly because people differ on what is considered hate speech and what is not.
%
\citet{balayn2021automatic} identified the mismatch between the formalisation of hate speech and how people perceive it.
%
Many factors influence how people perceive hate speech, such as the content itself and the characteristics of the target group and the observing individual, such as gender, cultural background, or age \citep{balayn2021automatic}.
%
We can identify this mismatch in other related work from which there appears to be low agreement among humans regarding annotating hate speech \citep{fortuna2018survey, ross2017measuring, waseem2016you}.
%
\citet{ross2017measuring} found low inter-rater reliability scores (Krippendorff's alpha values of around $0.2-0.3$) in a study where they asked humans about the hatefulness and offensiveness of 20 tweets.
%
They also found that the inter-rater reliability value does not increase when showing a definition of hate speech to the human annotators beforehand.
%
\citet{waseem2016you} found a slight increase in the inter-rater reliability when considering annotations of human experts only, but it remained low overall.
%
% This low agreement makes sense since there are many differences in people's personalities and backgrounds.
%
Therefore, hate speech detection is challenging, especially in computer science, since we have to be careful with bias.
%
Most annotated hate speech datasets that are publicly available contain bias.
%
Annotating hate speech datasets is challenging because social media data follows a skewed distribution since there are many more neutral social media posts than hateful ones \citep{fortuna2018survey}.
%
Datasets such as \citet{waseem2016hateful} or \citet{basile2019semeval} collected their data using specific keywords that can introduce \textit{sample retrieval} bias and annotated their data using only three independent annotators that might result in \textit{sample annotation} bias \citep{balayn2021automatic}.
%
Automated classification algorithms will likely become biased in their predictions if we train them on biased datasets.
%
Bias becomes most notable when applying pre-trained classification algorithms to new and unseen data in deployment.
%
For example, \citet{grondahl2018all} and \citet{arango2019hate} report significant drops in F1 scores when training a hate speech detection model on one dataset and evaluating it on another.
%
\citet{grondahl2018all} found that the F1 score reduces by 69\% in the worst case and that the model choice does not affect the classification performance as much as the dataset choice.
%
\citet{arango2019hate} replicated several state-of-the-art hate speech detection models and found that most studies overestimate the classification performance.
%
These results further strengthen our stance that we should not detect hate speech solely by machines but rather by a human-in-the-loop approach.

\section{Detection algorithms}
There is an increasing academic interest in the automatic detection of hate speech since the topic has become more relevant, as explained in the \nameref{ch:introduction}.
%
We will list the state-of-the-art Natural Language Processing (NLP) techniques for automatic hate speech detection from literature.
%
First, we will list the different features used in the classification approaches.
%
Then, we will list the currently used classification algorithms ranging from supervised to unsupervised learning.
%

%
Several excellent surveys outlined the classification approaches from literature  \citep{fortuna2018survey, schmidt2019survey}.
%
Commonly used features are bag-of-words (BOW) \citep{greevy2004classifying}, character/word N-grams \citep{waseem2016hateful}, lexicon features (e.g. using a word blacklist containing offensive slurs) \citep{xiang2012detecting},  term frequency-inverse document frequency (TF-IDF) \citep{badjatiya2017deep, davidson2017automated, rodriguez2019automatic}, part-of-speech (POS) \citep{greevy2004classifying}, sentiment analysis \citep{rodriguez2019automatic}, topic modelling (e.g. Latent Dirichlet Allocation (LDA)) \citep{xiang2012detecting}, meta-information (e.g. location) \citep{waseem2016hateful}, or word embeddings \citep{badjatiya2017deep}.
%
\citet{greevy2004classifying} found that the classification performance is higher with BOW features than with POS features.
%
\citet{waseem2016hateful} found that character N-gram achieves higher classification performance than word N-gram.
%
They also found that using demographic information such as the location does not improve the results significantly.
%
\citet{xiang2012detecting} used the topic distributions from an LDA analysis and a lexicon feature.
%
\citet{rodriguez2019automatic} used TF-IDF and sentiment analysis to detect and cluster topics on Facebook pages that are likely to promote hate speech.
%
\citet{badjatiya2017deep} experimented with different word embeddings: fastText\footnote{\url{https://fasttext.cc/}}, GloVe\footnote{\url{https://nlp.stanford.edu/projects/glove/}}, and random word embeddings.
%

Most hate speech-related studies use supervised learning techniques that range from traditional ML to deep learning classification models, and a few use unsupervised learning techniques to cluster social media posts.
%
Support Vector Machine (SVM) \citep{greevy2004classifying, xiang2012detecting,davidson2017automated} and Logistic Regression (LR) \citep{waseem2016hateful, davidson2017automated} are the most popular traditional ML techniques for hate speech detection.
%
\citet{davidson2017automated} found that SVM and LR perform significantly better than traditional ML techniques such as Naive Bayes, Decision Trees, and Random Forests.
%
\citet{badjatiya2017deep} experimented with various configurations of word embeddings and two deep learning models: a convolutional neural network (CNN) and a long short-term memory (LSTM) model.
%
They found that CNN performs better than LSTM and that using pre-trained word embeddings such as GloVe does not result in better classification performance than using random embeddings.
%
\citet{rodriguez2019automatic} use the unsupervised learning method, K-means clustering, to cluster social media posts to identify topics that potentially promote hate speech.
%


\section{Machine Learning models with rejection}
\todo[inline]{Explain the different architectures of ML with rejection}
\todo[inline]{Explain the different types of confidence metrics}
\todo[inline]{Explain the original metric from De Stefano}
\todo[inline]{Provide examples of ML models with rejection from other domains}

\section{Value assessment}
\todo[inline]{Explain what we mean by value and why we should integrate it into the design of a hate speech detection system.}


\todo[inline]{The authors of \citet{fjeld2020principled} outlined 8 principles of AI systems including fairness and discrimination (e.g. algorithmic bias), human control of technology (e.g. AI system should request help from the human user in difficult situations), and promotion of human values (we should integrate human value in the AI system).}

\todo[inline]{We need to weigh the value of (in)correct and rejected predictions into account in the design of a hybrid human-AI system \citep{sayin2021science}}

\todo[inline]{Value sensitive Design (VSD) from \citet{umbrello2021mapping} state that we can translate values such as freedom of bias into the design of a system. So example is a tax system that needs to detect fraud. If etniticity bias can be introduced by using postal codes, than we can exclude the postal code variable from the learning algorithm. We for example have the value reliability. The AI is not reliable sometimes, so we use reject option in our design to make the use of AI more reliable/. The authors say that we not only want to optimize the tax algotihm in terms of effectiveness (rate of fraud detection) but also in terms of fairness (presenting non-biased selection of cases. So the same holds in our case, we want to optimize in using the AI as much as possible but also want to take mistakes into account (in the end we still need to invovle humans to check specific cases).}

\todo[inline]{The Value-design algorithm paper from \citet{zhu2018value} describes 5 steps of algorithm design. We focus our approach on theirs by inspecting the stakeholders. And then assessing the stakeholder values to take this into account in the algoithm design.}



In this research, we focus on Machine Learning models with a reject option. The decision to accept or reject predictions depends heavily on the context. We argued in the \nameref{ch:introduction} that this decision should depend on the costs of incorrect predictions and the gains of correct predictions. We can express the costs of incorrect (FP and FN predictions) and rejected predictions as negative values. The gains of correct predictions (TP and TN predictions) as positive values. In some domains, we can define these values in money or time. For example, suppose there is a factory that uses a camera and an ML model to check if a package is damaged or not. Using an ML model will save the company time since these packages no longer have to be inspected manually by humans. However, the ML model could be incorrect sometimes. For example, a customer of the factory received a damaged package, while the ML model did not detect any damage. Fixing this issue could cost the factory money. At the same time, the factory could prevent these cases by rejecting the low confidence predictions from the ML model. For example, the ML model predicted with low confidence that a package did not contain any damage. An employee can then inspect it to prevent the customer from receiving a damaged one. Manually checking the rejected ones costs the factory a fraction of the time/money compared to the first situation. In this example, we can easily express the values of FP, FN, TP, TN, and rejections in time/money spent/saved.

However, it is not evident to express these values in the hate speech domain. Two stakeholders can be considered in the design of a smart rejector: the social media company and its users. We mainly focus on the users in this research since they will be affected the most by hate speech.

In this section, we will look at the related work to get an understanding of how we could retrieve the value ratios in hate speech detection. The goal is to retrieve ratios between rejection, FP, FN, TP, and TN cases. We would like to know whether an FN is, for example, two times worse compared to an FP. The main challenge is to express all values using a single unit. We could take two directions. First, we could define the values using an objective measure, such as time or money spent/saved. Second, we could define the values subjectively, e.g. by analyzing people's stance towards the consequence of incorrect predictions in hate speech detection. In the next two sections, we discuss the relevant related work in both directions.

\subsection{Objective assessment}
In this section, we explain the difficulties of defining the values using objective measurements. We do this by looking at some related work. We can retrieve the value of rejection by looking at how much time a human moderator spends on average to check whether some social media post contains hateful content or not. We can convert this into money by taking the moderator's salary into account. We could also argue that the value of a TP and a TN is equal to the negative value of rejection since we saved human effort by letting the ML model correctly predict whether something is hateful or not. The problem, however, starts to arise when we look at the FP and the FN predictions. How can we express the values of FP and FN predictions in terms of money or time?

First, we look at the social media company as a stakeholder. As we explained in the previous section, the values of rejection, TP, and TN can be determined. So the values of FP and FN are yet to be defined. However, most social media companies are not transparent in how they moderate hate speech \citep{klonick2017new}. So we do not have clear insights into the costs for these companies. There do exist country-specific fines. For example, Germany approved a plan where social media companies can be fined up to 50 million euros if they do not remove hate speech in time \citep{bbc-firms-face-fine-germany}. However, this is location-specific, and it is unclear how this applies to individual cases of hate speech. Defining the value of FP cases is even more difficult. It is unclear how filtering out too much content would affect the company (apart from many annoyed users whose freedom of speech is violated). Therefore, we abstain from estimating the values from the perspective of these companies.

Second, both FP and FN predictions have consequences on the users as the stakeholder. Having too many FP predictions might violate the value of Freedom of Speech since we are filtering out non-hateful posts and, therefore, we cause suppression of free speech. One paper found through a survey that most people think that some form of hate speech moderation is needed, but they also worry about the violation of freedom of speech \citep{olteanu2017limits}. Having too many FN predictions might harm individuals or even result in acts of violence \citep{ecri-hate-speech-and-violence}. Therefore, we need to figure out how we should weigh the values of FP and FN predictions accordingly. We abstain from using time as a unit since it does not make sense to express the consequences of hate speech or the benefits of freedom of speech in time. Therefore, we want to look at the value of freedom of speech and hate speech from an economic perspective. However, we noticed a lack of research in this area. There is one paper where they tried to come up with an economic model for free political speech by looking at the First Amendment to the United States Constitution \citep{posner1986free}. The First Amendment restricts the government from creating laws that could, for example, violate Freedom of Speech \citep{first-amendment-white-house}. The authors explained in \citet{posner1986free} that the lack of research in this area is because most economists do not dive into the legal domain regarding free speech, and free speech legal specialists refrain from doing economic analysis \citep{posner1986free}. The proposed economic model from the paper, for example, includes the cost of harm and the probability that speech results in violence \citep{posner1986free}. However, the authors do not elaborate on how we can define the probability and the costs. Another paper did speculate on this topic by explaining why doing a cost-benefit analysis of free speech is almost impossible \citep{sunstein2018does}. The authors explained that there are too many uncertainties \citep{sunstein2018does}. We can assume that there are values of free speech, but it is too difficult to quantify them \citep{sunstein2018does}. For example, terrorist organizations use free speech to recruit people and call for acts of violence online \citep{sunstein2018does}. At the same time, most other hateful posts will not ever result in actual acts of violence \citep{sunstein2018does}. Therefore, cost values using objective measurements are often case-specific and cannot be defined generically. There is a nonquantifiable risk that acts of violence will happen in the unknown future \citep{sunstein2018does}. But suppose we do know this probability, then there are still too many uncertainties. To calculate the actual costs of hate speech (in our case: to accept the FN predictions), we also need to know the number of lives at risk and how we should quantify the value of each life \citep{sunstein2018does}? The authors claim that analyzing the benefits of free speech is even more difficult \citep{sunstein2018does}. They conclude their work by saying that there are too many problems to empirically evaluate the costs and benefits in the hate speech context \citep{sunstein2018does}.

Therefore, we believe that using objective measurements, such as money, is not realistic for generically expressing the cost values in our project for both stakeholders.

\subsection{Subjective assessment}
- Focus on subjective values of users
- Not companies


\section{Evaluation metrics}
Most hate speech-related studies evaluate their classification methods using standard \textit{machine} metrics such as accuracy, precision, recall, or F1.
%
\citet{rottger2020hatecheck} recognized the shortcomings of these metrics since it is hard to recognize the weak points of classification models.
%
Therefore, the authors presented a suite of 29 carefully selected functional tests to help identify the model's weaknesses \citep{rottger2020hatecheck}.
%
Each test checks different criteria, such as the ability to cope with spelling variations or the ability to detect neutral content that contain slurs \citep{rottger2020hatecheck}.
%
We believe this is a step in the right direction of evaluating and improving hate speech detection models.
%
However, our approach is different since we do not solely focus on evaluating the detection performance of the underlying classification model but instead on evaluating models with a reject option while taking into account the human-perceived value of (in)correct and rejected predictions.
%
Examples include outlining ethical principles of algorithmic systems (Fjeld et al. 2020), developing value-based assessment frameworks (Yurrita et al. 2022), and proposing new metrics for evaluating machine learning systems that incorporate value parameters (Casati, No ̈ el, and Yang 2021)

\todo[inline]{Explain that \cite{olteanu2017limits} claims that we need human-centred metrics instead of abstract metrics such as precision}

\todo[inline]{"proposing new metrics for evaluating machine learning systems that incorporate value parameters \citep{casati2021value}"}