\chapter{Related work}
\todo[inline]{This section gives some background information about ML with rejection, hate speech detection, and unknown unknown detection}

\section{Hate speech detection}
In this section, we first briefly introduce the definition of hate speech and the challenges of detecting it. Then, we provide an overview of some methods commonly used for automatically detecting hate speech.

Different types of online conflictual languages exist, such as cyberbullying, offensive language, toxic language, or hate speech, and come with different definitions from domains such as psychology, political science, or computer science \citep{balayn2021automatic}.
%
We can broadly define hate speech as ``language that is used to express hatred towards a targeted group or is intended to be derogatory, to humiliate, or to insult the members of the group'' \citep{davidson2017automated, balayn2021automatic}.
%
It differs from other conflictual languages since it's mostly focused on specific target groups or individuals \citep{balayn2021automatic}.
%
But there exist many more definitions of hate speech in literature mainly because there is a lot of discussion about what is considered hate speech and what not.
%
From several studies, it appears that there is low agreement among humans when it comes to annotating hate speech \citep{fortuna2018survey, ross2017measuring, waseem2016you}.
%
\citet{ross2017measuring} found low inter-rater reliability scores (Krippendorff's alpha values of around $0.2-0.3$) in a study where they asked humans about the hatefulness and offensiveness of 20 tweets.
%
They also found that the inter-rater reliability value does not increase when showing a definition of hate speech to the human annotators before the survey.
%
\citet{waseem2016you} found a slight increase in the inter-rater reliability when considering annotations of human experts only, but it remained low overall.
%
This low agreement makes sense since there are many differences in people's personalities and backgrounds.
%
Therefore, hate speech detection is challenging, especially in computer science, since we have to be careful with bias.
%
Most annotated hate speech datasets that are publicly available contain bias.
%
Datasets such as \citet{waseem2016hateful} or \citet{basile2019semeval} collected their data using specific keywords that can introduce \textit{sample retrieval} bias and annotated their data using only three independent annotators that might result in \textit{sample annotation} bias \citep{balayn2021automatic}.
%
Automated classification algorithms are likely to become biased in their predictions if they are trained on biased datasets.
%
Bias becomes most notable when applying pre-trained classification algorithms to new and unseen data in deployment.
%
For example, \citet{grondahl2018all} and \citet{arango2019hate} report significant drops in F1 scores when training a hate speech detection model on one dataset and evaluating it on another.
%
These results further strengthen our stance that hate speech detection cannot and should not be done by machines only but rather by a human-in-the-loop approach.


\section{Machine Learning models with rejection}
\todo[inline]{Explain the different architectures of ML with rejection}
\todo[inline]{Explain the different types of confidence metrics}
\todo[inline]{Explain the original metric from De Stefano}
\todo[inline]{Provide examples of ML models with rejection from other domains}


\section{Cost assessment in hate speech detection}
\todo[inline]{Explain how we could do cost analysis in hate speech detection and why some methods do not work (such as expressing the costs in money or time) and which methods might work (such as using surveys for retrieving subjective costs). Also, explain what Magnitude Estimation is and provide examples of studies that used it to retrieve subjective judgements.}

In this research, we focus on Machine Learning models with a reject option. The decision to accept or reject predictions depends heavily on the context. We argued in the \nameref{ch:introduction} that this decision should depend on the consequences of incorrect predictions and the gains of correct predictions. We can express the cost of incorrect predictions as the cost of an FP and an FN. The gains as the gain of a TP and a TN. In some domains, we can define these gain/cost values in money or time. For example, suppose there is a factory that uses a camera and an ML model to check if a package is damaged or not. Using an ML model will save the company time since these packages no longer have to be inspected manually by humans. However, the ML model could be incorrect sometimes. For example, a customer of the factory received a damaged package, while the ML model did not detect any damage. Fixing this issue could cost the factory money. At the same time, the factory could prevent these cases by rejecting the low confidence predictions from the ML model. For example, the ML model predicted with low confidence that a package did not contain any damage. An employee can then inspect it to prevent the customer from receiving a damaged one. Manually checking the rejected ones costs the factory a fraction of the time/money compared to the first situation. In this example, we can easily express the cost/gain values of FP, FN, TP, TN, and rejections in time/money spent/saved.

However, it is not evident to express these costs in the hate speech domain. Two stakeholders can be considered in the design of a smart rejector: the social media company and its users. We mainly focus on the users in this research since they will be affected the most by hate speech.

In this section, we will look at the related work to get an understanding of how we could retrieve the relative cost values in hate speech detection. The goal is to retrieve relative cost values for rejection, FP, FN, TP, and TN cases. We would like to know whether an FN is, for example, two times worse compared to an FP. The main challenge is to express the cost values using a single unit. We could take two directions. First, we could define the costs using an objective measure, such as time or money spent/saved. Second, we could define the costs subjectively, e.g. by analyzing people's stance towards the consequence of incorrect predictions in hate speech detection. In the next two sections, we discuss the relevant related work in both directions.

\subsection{Objective costs}
In this section, we explain the difficulties of defining the cost values using objective measurements. We do this by looking at some related work. We can retrieve the cost of rejection by looking at how much time a human moderator spends on average to check whether some social media post contains hateful content or not. We can convert this into money by taking the moderator's salary into account. We could also argue that the gain of a TP and a TN is equal to the inverse cost of rejection since we saved human effort by letting the ML model correctly predict whether something is hateful or not. The problem, however, starts to arise when we look at the FP and the FN predictions. How can we express the costs of FP and FN predictions in terms of money or time?

First, we look at the social media company as a stakeholder. As we explained in the previous section, the costs of rejection, TP, and TN can be determined. So the costs of FP and FN are yet to be defined. However, most social media companies are not transparent in how they moderate hate speech \citep{klonick2017new}. So we do not have clear insights into the costs for these companies. There do exist country-specific fines. For example, Germany approved a plan where social media companies can be fined up to 50 million euros if they do not remove hate speech in time \citep{bbc-firms-face-fine-germany}. However, this is location-specific, and it is unclear how this applies to individual cases of hate speech. Defining the cost of FP cases is even more difficult. It is unclear how filtering out too much content would affect the company (apart from many annoyed users whose freedom of speech is violated). Therefore, we abstain from estimating the costs from the perspective of these companies.

Second, both FP and FN predictions have consequences on the users as the stakeholder. Having too many FP predictions might violate the value of Freedom of Speech since we are filtering out non-hateful posts and, therefore, we cause suppression of free speech. One paper found through a survey that most people think that some form of hate speech moderation is needed, but they also worry about the violation of freedom of speech \citep{olteanu2017limits}. Having too many FN predictions might harm individuals or even result in acts of violence \citep{ecri-hate-speech-and-violence}. Therefore, we need to figure out how we should weigh the costs of FP and FN predictions accordingly. We abstain from using time as a unit since it does not make sense to express the consequences of hate speech or the benefits of freedom of speech in time. Therefore, we want to look at the gains/costs of freedom of speech and hate speech from an economic perspective. However, we noticed a lack of research in this area. There is one paper where they tried to come up with an economic model for free political speech by looking at the First Amendment to the United States Constitution \citep{posner1986free}. The First Amendment restricts the government from creating laws that could, for example, violate Freedom of Speech \citep{first-amendment-white-house}. The authors explained in \citet{posner1986free} that the lack of research in this area is because most economists do not dive into the legal domain regarding free speech, and free speech legal specialists refrain from doing economic analysis \citep{posner1986free}. The proposed economic model from the paper, for example, includes the cost of harm and the probability that speech results in violence \citep{posner1986free}. However, the authors do not elaborate on how we can define the probability and cost values. Another paper did speculate on this topic by explaining why doing a cost-benefit analysis of free speech is almost impossible \citep{sunstein2018does}. The authors explained that there are too many uncertainties \citep{sunstein2018does}. We can assume that there are costs and benefits of free speech, but it is too difficult to quantify them \citep{sunstein2018does}. For example, terrorist organizations use free speech to recruit people and call for acts of violence online \citep{sunstein2018does}. At the same time, most other hateful posts will not ever result in actual acts of violence \citep{sunstein2018does}. Therefore, cost values using objective measurements are often case-specific and cannot be defined generically. There is a nonquantifiable risk that acts of violence will happen in the unknown future \citep{sunstein2018does}. But suppose we do know this probability, then there are still too many uncertainties. To calculate the actual costs of hate speech (in our case: to accept the FN predictions), we also need to know the number of lives at risk and how we should quantify the value of each life \citep{sunstein2018does}? The authors claim that analyzing the benefits of free speech is even more difficult \citep{sunstein2018does}. They conclude their work by saying that there are too many problems to empirically evaluate the costs and benefits in the hate speech context \citep{sunstein2018does}.

Therefore, we believe that using objective measurements, such as money, is not realistic for generically expressing the cost values in our project for both stakeholders.

\subsection{Subjective costs}


\section{Unknown (un)known detection}
\todo[inline]{Give examples of existing unknown (un)known detection methods from literature}

