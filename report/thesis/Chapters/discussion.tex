\chapter{Discussion}
The main goal of this project was to propose a way of rejecting ML model predictions in a value-sensitive manner for hate speech detection.
%
We split this up into two parts.
%
First, we wanted to determine how we could measure the total value of ML models with a reject option.
%
We proposed a value-sensitive metric that measures the total value of an ML model for some rejection threshold, where we reject and accept all predictions with a confidence value below and above the threshold, respectively.
%
This calculation is based on a set of predictions for some datasets and the value ratios between TP, TN, FP, FN, and rejected predictions.
%
By maximizing the total value, we can find the optimal rejection threshold.
%
Second, we wanted to develop a method for determining these value ratios necessary for the metric's calculations.
%
We proposed to estimate the value ratios in a large survey study using the ME scale.
%
The results from chapter \ref{ch:results} showed several key findings:
\begin{itemize}
    \item The survey's results indicated that the ME technique is suitable for retrieving the value ratios as the results passed both the reliability and validity analyses.
    \item We did not observe significant statistical differences for most scenarios between groups of participants with different demographical characteristics.
    \item The experiments with the \emph{seen} data showed that our value-sensitive rejector could be beneficial for detecting hate speech when we consider not rewarding correct predictions.
    \item The experiments with the \emph{unseen} data demonstrated that hate speech detection models are susceptible to bias, affecting the results of our value-sensitive rejector since we had to reject more predictions when we considered not rewarding correct predictions.
\end{itemize}
%

%
This chapter analyzes the results from chapter \ref{ch:results} in greater detail.
%
First, we discuss the main findings of the survey study in section \ref{sec:discussion-survey} and our value-sensitive rejector in section \ref{sec:discussion-rejection}.
%
Finally, we highlight some limitations of our approach in section \ref{sec:discussion-limitations} and give some recommendations in section \ref{sec:discussion-recommendations}.


\section{Survey study}
\label{sec:discussion-survey}
%
In each scenario, we first asked the participant to indicate if they thought the social media post was hateful or not.
%
We found three scenarios for which more than 40\% disagreed with the ground truth label (FN5, REJ5, and REJ7).
%
We also recognized this in the ME response values since participants generally were neutral about these scenarios.
%
Scenarios FN5 and REJ5 are annotated as non-aggressive hate speech targeted at a generic group and seem less hateful than the other posts, and the post in REJ7 contains an offensive slur.
%
Given the nature of these social media posts used in these scenarios, it might explain the larger disagreement about annotating it as hateful/non-hateful between participants for these scenarios.
%

%
We simulated the TP, TN, FP, FN, or rejection scenarios by asking the second and third questions where the participant had to provide a response value using either the ME or the 100-level scale to express their agreement or disagreement with SocialNet's decision.
%
We analyze the resulting response values by looking at three aspects.
%
First, we analyze the value ratios from the survey that uses the ME scale in section \ref{sec:discussion-value-ratios}.
%
Then we discuss whether the ME technique passes the reliability and validity analyses in sections \ref{sec:discussion-reliability} and \ref{sec:discussion-validity}.
%
Finally, we analyze the results of the demographic analysis in section \ref{sec:discussion-demographic}


\subsection{Value ratios}
\label{sec:discussion-value-ratios}
%
Regarding the value ratios, most results are in line with our hypotheses.
%
The resulting values of both the ME and the 100-level scale follow the same order ($V_{fn} < V_{fp} < V_{r} < V_{tp} < V_{tn}$).
%
We noticed that participants disagreed the most with scenarios FN3 and FN7.
%
According to the annotations given by \citet{basile2019semeval}, both scenarios are hate speech targeted at an individual and contain aggressive speech.
%
The results of FN3 and FN7 might suggest that participants disagree the most with FN predictions for aggressive hate speech target to individuals.
%
As expected, we found that participants disagree with the FP, FN, and rejected predictions, that the value of an FN is lower than an FP, and that the average value of an FP and an FN is lower than the rejection value.
%
The results show that participants find a hateful post that is not detected worse than a non-hateful post detected as hate speech.
%
This finding is in line with our hypothesis that tolerating hate speech (FN predictions) negatively impacts social media users more than forbidding neutral speech (FP predictions).
%
The value of rejection is the closest to 0 (neutral) because rejected predictions do not cause any benefit or harm but only reduce the value by the human moderation effort.
%
However, two things were somewhat surprising.
%
First, participants appreciated correct predictions more than incorrect predictions since participants gave higher absolute values to TP and TN scenarios than FP and FN scenarios.
%
We expect participants to give lower absolute response values to correct predictions since it is expected from the automatic detection algorithms to produce correct predictions.
%
However, we look at this from a computer science perspective, where we want to prevent incorrect predictions, whereas the participants might think producing correct predictions is more critical.
%
Second, we were surprised that the TN value was greater than TP, while we expected the opposite to hold.
%
One possible reason could be that people disagree more on what is considered hateful among the TP scenarios, as seen in figure \ref{fig:hatefulness}.
%
This observation is in line with the findings of \citet{ross2017measuring}, as the authors found low agreement among participants regarding labelling social media posts as hate speech.
%

\subsection{Reliability analysis}
\label{sec:discussion-reliability}
%
According to the Krippendorff's alpha values ($\alpha$), the results of the ME scale are reliable, indicating that the ME technique is suitable for estimating the value ratios.
%
Contrary to our hypothesis, the results indicated that the 100-level scale is less reliable than the ME scale.
%
We would expect many participants to give response values of -100, 100, or 0 as the 100-level scale is bounded, and, therefore, we would expect higher alpha values for the 100-level scale compared to the ME scale.
%
The results also showed low alpha values when we computed it for each group of scenarios with the same type (TP, TN, FP, FN, and rejection).
%
Users tend to agree more on incorrect predictions than on the correct predictions, indicating that participants disagree more on what constitutes hate speech in the first place.
%
This phenomenon is in line with the findings of \citet{ross2017measuring}, where the authors also found low Krippendorff's alpha values when asking participants to annotate hate speech.
%
The alpha values for the grouped scenarios by type are lower compared to when we consider all scenarios.
%
We can explain this phenomenon by looking at the calculation of Krippendorff's alpha.
%
In this calculation, we measure the difference between the expected difference and the observed difference.
%
When we consider the response values to all scenarios, the values tend to follow the same trend; positive values to correct predictions and negative values for incorrect and rejected predictions.
%
When we consider the response values to the scenarios of the same type, e.g. all TP scenarios, the values seem more randomly distributed as each participant uses a different positive response value to the TP scenarios.
%
Therefore, when considering all scenarios, the observed difference between the response values is closer to the difference expected by chance, resulting in higher alpha values.
%

%
\subsection{Validity analysis}
\label{sec:discussion-validity}
The cross-modality validation between the ME and the 100-level scales showed that the response values to both scales are highly correlated, indicating that we validated the ME technique for measuring people's opinions about different hate speech detection scenarios.
%
The S-shaped curve in figure \ref{fig:correlation} is because for two reasons.
%
First, the magnitude estimates are skewed towards 0 because of the normalization procedure.
%
Second, the 100-level scores are skewed towards the upper and lower bounds of 100 and -100 as the participants are more likely to assign the highest or lowest value.

\subsection{Demographic analysis}
\label{sec:discussion-demographic}
\todo[inline]{Discuss why FP7 and REJ4 have most sig. differences among all features. And why TP6, FN5, and REJ1 have the second most differences. Not clear why FP7. But REJ4 is a neutral tweet about a political topic. People could have different opinions about refugees espcially from different nationalities. This might explain the differences.}
\todo[inline]{Only FN5 has the most pairwise sig. differences, which is also a tweet about the building the wall.}
\todo[inline]{However, when looking at the other scenarios, we mainly see that there are no differences between groups with different demographic features.}
\todo[inline]{For the sex feature, we do not find any differences between the men and women. This is what we expected as related work also found this (refer to work).}
\todo[inline]{Furthermore, for the features with multiple groups (more than two) where we did find sig. differences, we often do not find any pairwise sig. differences between the groups.}
\todo[inline]{Therefore, we can conclude that for our dataset, people with different demographic characterisitcs overall tend to give the same judgements.}
\todo[inline]{However, we can see that differences between groups of participants for features such as nationality, language, or ethnicity are greater than features such as sex or student.}
\todo[inline]{The results of both the Kruskal-Wallis and the pairwise Mann-Whitney U tests are very similar between the nationality and language features. This can be explained by the fact that the groups of both features are very similar as only a couple of participants differ between the two features.}
\todo[inline]{Also, we see that there are more hateful posts (10) than non-hateful ones (5) with significant differences, meaning that hateful tweets are more sensitive to trigger differences between groups with different demographic characterisitcs.}
\todo[inline]{Also, we should keep in mind that because of the clustering analysis, we created a selection of tweets as diverse as possible. So some posts may not result in any differences between groups while others do. For example, there are five posts, both hateful and non-hateful, about building a wall in America across the Mexican border and four posts where there is at least one feature with significant differences between the groups.}
\todo[inline]{Therefore, we can conclude that the significant differences between groups for a scenario depends highly on the content of the social media post.}

\section{Value-sensitive rejection}
\label{sec:discussion-rejection}

\section{Implications}
\todo[inline]{Explain that \cite{olteanu2017limits} claims that we need more human-centred metrics instead of abstract metrics such as precision and we agree with that by introducing our own human-centred metric}


\todo[inline]{Answer research questions}

\section{Limitations}
\label{sec:discussion-limitations}
\todo[inline]{Hate speech is difficult domain as there tend to be a lot of disagreement between people about what is considered hate speech and what not. \citet{ross2017measuring} found low Krippendorff alpha values in a hate speech survey. So our findings are in line with theirs.}
\todo[inline]{Explain limitations of the metric and the survey study}
\todo[inline]{Finally, we should point out the limitations of the demographic analysis.}
\todo[inline]{First, the sample sizes of the demographic groups are not representative of the population of those groups.}
\todo[inline]{For example, for the nationality feature, we had five participants from Spain.}
\todo[inline]{Second, we have to keep in mind that some features where we did find sig. differences might have happened by chance. There may have been participants that did not understood the scenario, either because the lack of english or because they rushed through the survey.}
\todo[inline]{We represented the scenarios by explaining that we would rank social media posts lower higher on people's feeds. Initially, we explained in the pilot survey that we would remove or keep posts. However, we noticed larger disagreement values in the FP and TP cases since apparantly some might think that we should allow everything on the web even hateful posts. Therefore, we are aware that the decisions of social media platforms on how to deal with detected hateful posts impacts people's opinions about it.}
\todo[inline]{The rejection threshold is calculated using the test set. This test set needs to be as realistic as possible. Furthermore we need to have calibrated models since we rely purely on the confidence values. This is also hard to realize. Temperature scaling can help, but it is still limited.}

\section{Recommendations}
\label{sec:discussion-recommendations}
\todo[inline]{Magnitude Estimation seems promising for future research in HCI.}
\todo[inline]{Personal and demographic characterisitcs might have  a big impact. So further analysis on those aspects seem relevant.}
\todo[inline]{Perhaps we can train ML models using the values of TP, TN, FP, FN, rejection in an integrated rejector. So we train the ML model and the rejector simultanously using the values from the survey. So then during training, the FN predictions are punished more than FP predictions.}