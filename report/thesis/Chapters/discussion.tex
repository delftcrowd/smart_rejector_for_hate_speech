\chapter{Discussion}
The main goal of this project was to propose a way of rejecting ML model predictions in a value-sensitive manner for hate speech detection.
%
We split this up into two parts.
%
First, we wanted to determine how we could measure the total value of ML models with a reject option.
%
We proposed a value-sensitive metric that measures the total value of an ML model for some rejection threshold, where we reject and accept all predictions with a confidence value below and above the threshold, respectively.
%
This calculation is based on a set of predictions for some datasets and the value ratios between TP, TN, FP, FN, and rejected predictions.
%
By maximizing the total value, we can find the optimal rejection threshold.
%
Second, we wanted to develop a method for determining these value ratios necessary for the metric's calculations.
%
We proposed to estimate the value ratios in a large survey study using the ME scale.
%
The results from chapter \ref{ch:results} showed several key findings:
\begin{itemize}
    \item The results indicated that the ME technique is suitable for retrieving the value ratios as the results passed both the reliability and validity analyses.
    \item We did not observe significant statistical differences between groups of participants for most scenarios and demographical features.
    \item The results showed that our value-sensitive rejector could be beneficial for detecting hate speech when we consider punishing incorrect predictions only.
    \item The experiments with the unseen data demonstrated that hate speech detection models are susceptible to bias, affecting the results of our value-sensitive rejector since we had to reject more predictions when we considered punishing incorrect predictions only.
\end{itemize}
%

%
This chapter analyzes the results from chapter \ref{ch:results} in greater detail.
%
First, we discuss the main findings of the survey study in section \ref{sec:discussion-survey} and our value-sensitive rejector in section \ref{sec:discussion-rejection}.
%
Finally, we highlight some limitations of our approach in section \ref{sec:discussion-limitations} and give some recommendations in section \ref{sec:discussion-recommendations}.


\section{Survey study}
\label{sec:discussion-survey}
%
The first question of each scenario in the survey was informative and asked the participants whether they think the social media post is hateful or not.
%
We found three scenarios for which more than 40\% disagreed with the ground truth label (FN5, REJ5, and REJ7).
%
We also recognized this in the ME response values since participants generally were neutral about these scenarios.
%
Scenarios FN5 and REJ5 are annotated as non-aggressive hate speech targeted at a generic group and seem less hateful than the other posts, and the post in REJ7 contains an offensive slur.
%
Given the nature of the social media posts used in these scenarios, it might explain the larger disagreement about annotating it as hateful/non-hateful between participants for these scenarios.
%

%
We used the second and third questions of each scenario in the survey for determing the value ratios.
%
Regarding value ratios, most results are in line with our hypotheses.
%
The resulting values of both the ME and the 100-level scale follow the same order ($V_{fn} < V_{fp} < V_{r} < V_{tp} < V_{tn}$).
%
We noticed that participants disagreed the most with scenarios FN3 and FN7.
%
According to the annotations given by \citet{basile2019semeval}, both scenarios are hate speech targeted at an individual and contain aggressive speech.
%
The results of FN3 and FN7 might suggest that participants disagree the most with FN predictions for aggressive hate speech target to individuals.
%
As expected, we found that participants disagree with the FP, FN, and rejected predictions, that the value of an FN is lower than an FP, and that the average value of an FP and an FN is lower than the rejection value.
%
The results show that participants find a hateful post that is not detected worse than a non-hateful post that is detected as hate speech.
%
This is in line with our assumption of tolerating hate speech (FN) to have a more negative impact on the social media users than forbidding neutral speech (FP).
%
The value of rejection is the closest to 0 (neutral), because rejected predictions do not cause any benefit or harm but only reduce the value by the human moderation effort.
%
However, there are two things that were somewhat supprising.
%
First, participants appreciated correct predictions more than incorrect predictions since participants gave higher absolute values to TP and TN than FP and FN.
%
We expect participants to give lower absolute response values to correct predictions since it is expected from the automatic detection algorithms to produce correct predictions.
%
However, we look at this from a computer science perspective where we want to prevent incorrect predictions whereas the participants might think that correct predictions are more important.
%
Second, we were suprised to see that the value of a TN is greater than of a TP while we expected the opposite to hold.
%
One possible reason could be that people disagree more on what is considered hateful among the TP scenarios, as can be seen in figure \ref{fig:hatefulness}.
%
This is in line with the findings of \citet{ross2017measuring} as the authors found low agreement among participants when it comes to labeling social media posts as ahte speech.
%

%
Regarding the reliability analysis, according to the Krippendorff's alpha values ($\alpha$), the results of the ME scale are reliable, indicating that the ME technique is suitable for estimating the value ratios.
%
In contrary with our hypothesis, the results indicated that the 100-level scale is less reliable than the ME scale.
%
We would expect many participants to give response values of -100, 100, or 0 as the 100-level scale is bounded, and, therefore, we would expect higher alpha values for the 100-level scale compared to the ME scale.
%
The results also showed low alpha values when we compute it for each group of scenarios with the same type (TP, TN, FP, FN, and rejection).
%
Users tend to be more in agreement on incorrect predictions than on the correct predictions, indicating that participants disagree more on what constitutes hate speech in the first place.
%
This is in line with the findings of \citet{ross2017measuring} where the authors also found low Krippendorff's alpha values when asking participants to annotate hate speech.
%
The alpha values for the scenario types are lower compared to when we consider the complete scale.
%
We can explain this phenomenon by looking at the calculation of Krippendorff's alpha.
%
In this calculation, we measure the difference between the expected difference and the observed difference.
%
When we consider the response values to all scenarios, the values tend to follow the same trend; positive values to correct predictions and negative values to incorrect and rejected predictions.
%
When we consider the response values to the scenarios of the same type, e.g. all TP scenarios, the values seem more randomly distributed as each participant uses a different positive response value to the TP scenarios.
%
Therefore, when considering the complete scale, the observed difference between the response values is closer to the difference expected by chance, resulting into higher alpha values.
%

%


\todo[inline]{Discuss why FP7 and REJ4 have most sig. differences among all features. And why TP6, FN5, and REJ1 have the second most differences. Not clear why FP7. But REJ4 is a neutral tweet about a political topic. People could have different opinions about refugees espcially from different nationalities. This might explain the differences.}
\todo[inline]{Only FN5 has the most pairwise sig. differences, which is also a tweet about the building the wall.}
\todo[inline]{However, when looking at the other scenarios, we mainly see that there are no differences between groups with different demographic features.}
\todo[inline]{For the sex feature, we do not find any differences between the men and women. This is what we expected as related work also found this (refer to work).}
\todo[inline]{Furthermore, for the features with multiple groups (more than two) where we did find sig. differences, we often do not find any pairwise sig. differences between the groups.}
\todo[inline]{Therefore, we can conclude that for our dataset, people with different demographic characterisitcs overall tend to give the same judgements.}
\todo[inline]{However, we can see that differences between groups of participants for features such as nationality, language, or ethnicity are greater than features such as sex or student.}
\todo[inline]{The results of both the Kruskal-Wallis and the pairwise Mann-Whitney U tests are very similar between the nationality and language features. This can be explained by the fact that the groups of both features are very similar as only a couple of participants differ between the two features.}
\todo[inline]{Also, we see that there are more hateful posts (10) than non-hateful ones (5) with significant differences, meaning that hateful tweets are more sensitive to trigger differences between groups with different demographic characterisitcs.}
\todo[inline]{Also, we should keep in mind that because of the clustering analysis, we created a selection of tweets as diverse as possible. So some posts may not result in any differences between groups while others do. For example, there are five posts, both hateful and non-hateful, about building a wall in America across the Mexican border and four posts where there is at least one feature with significant differences between the groups.}
\todo[inline]{Therefore, we can conclude that the significant differences between groups for a scenario depends highly on the content of the social media post.}

\section{Value-sensitive rejection}
\label{sec:discussion-rejection}

\section{Implications}
\todo[inline]{Explain that \cite{olteanu2017limits} claims that we need more human-centred metrics instead of abstract metrics such as precision and we agree with that by introducing our own human-centred metric}


\todo[inline]{Answer research questions}

\section{Limitations}
\label{sec:discussion-limitations}
\todo[inline]{Hate speech is difficult domain as there tend to be a lot of disagreement between people about what is considered hate speech and what not. \citet{ross2017measuring} found low Krippendorff alpha values in a hate speech survey. So our findings are in line with theirs.}
\todo[inline]{Explain limitations of the metric and the survey study}
\todo[inline]{Finally, we should point out the limitations of the demographic analysis.}
\todo[inline]{First, the sample sizes of the demographic groups are not representative of the population of those groups.}
\todo[inline]{For example, for the nationality feature, we had five participants from Spain.}
\todo[inline]{Second, we have to keep in mind that some features where we did find sig. differences might have happened by chance. There may have been participants that did not understood the scenario, either because the lack of english or because they rushed through the survey.}

\todo[inline]{The rejection threshold is calculated using the test set. This test set needs to be as realistic as possible. Furthermore we need to have calibrated models since we rely purely on the confidence values. This is also hard to realize. Temperature scaling can help, but it is still limited.}

\section{Recommendations}
\label{sec:discussion-recommendations}
\todo[inline]{Magnitude Estimation seems promising for future research in HCI.}
\todo[inline]{Personal and demographic characterisitcs might have  a big impact. So further analysis on those aspects seem relevant.}
\todo[inline]{Perhaps we can train ML models using the values of TP, TN, FP, FN, rejection in an integrated rejector. So we train the ML model and the rejector simultanously using the values from the survey. So then during training, the FN predictions are punished more than FP predictions.}