\chapter{Discussion}
\label{ch:discussion}
The main goal of this project was to propose a way of rejecting ML model predictions in a value-sensitive manner for hate speech detection.
%
We split this up into two parts.
%
First, we wanted to determine how we could measure the total value of ML models with a reject option.
%
We proposed a value-sensitive metric that measures the total value of an ML model for some rejection threshold, where we reject and accept all predictions with a confidence value below and above the threshold, respectively.
%
This calculation is based on a set of predictions and the value ratios between TP, TN, FP, FN, and rejected predictions.
%
By maximizing the total value, we can find the optimal rejection threshold.
%
Second, we wanted to develop a method for determining these value ratios necessary for the metric's calculations.
%
We proposed to estimate the value ratios in a large survey study using the ME scale.
%
The results from chapter \ref{ch:results} showed several key findings:
\begin{itemize}
    \item The survey's results indicated that the ME technique is suitable for retrieving the value ratios from human ratings to hate speech detection scenarios since the results passed both the reliability and validity analyses.
    \item Social media users appreciate correct predictions more than they detest incorrect predictions, especially content correctly identified as non-hateful and, therefore, not banned from the platform.
    \item Social media users agree more on rating the negative value of incorrect predictions than the positive value of correct predictions.
    \item We did not observe significant statistical differences for most scenarios between groups of participants with different demographical characteristics.
    \item The experiments with the \emph{seen} data showed that our value-sensitive rejector maximizes the utility of hate speech detection models in terms of the value of our value-sensitive metric when we consider not rewarding correct predictions.
    \item The experiments with the \emph{unseen} data demonstrated that hate speech detection models are susceptible to bias, affecting the results of our value-sensitive rejector since we had to reject more predictions when we considered not rewarding correct predictions. Also, the results showed that when using our value-sensitive metric, the best model selected can be different compared to using accuracy.
\end{itemize}
%

%
This chapter analyzes the results from chapter \ref{ch:results} in greater detail.
%
First, we discuss the main findings of the survey study in section \ref{sec:discussion-survey} and our value-sensitive rejector in section \ref{sec:discussion-rejection}.
%
Finally, we highlight some limitations of our approach in section \ref{sec:discussion-limitations} and give some recommendations in section \ref{sec:discussion-recommendations}.


\section{Survey study}
\label{sec:discussion-survey}
%
In each scenario, we first asked the participant to indicate if they thought the social media post was hateful or not.
%
We found three scenarios for which more than 40\% disagreed with the ground truth label (FN5, REJ5, and REJ7).
%
We also recognized this in the ME response values since participants generally were neutral about these scenarios.
%
Scenarios FN5 and REJ5 are annotated as non-aggressive hate speech targeted at a generic group and seem less hateful than the other posts, and the neutral post in REJ7 contains an offensive slur.
%
Given the nature of these social media posts used in these scenarios, it might explain the larger disagreement between participants in annotating it as hateful/non-hateful for these scenarios.
%

%
We simulated the TP, TN, FP, FN, and rejection scenarios by asking the second and third questions where the participant had to provide a response value using either the ME or the 100-level scale to express their agreement or disagreement with SocialNet's decision.
%
We analyze the resulting response values by looking at three aspects.
%
First, we analyze the value ratios from the survey that uses the ME scale in section \ref{sec:discussion-value-ratios}.
%
Then we discuss whether the ME technique passes the reliability and validity analyses in sections \ref{sec:discussion-reliability} and \ref{sec:discussion-validity}.
%
Finally, we analyze the results of the demographic analysis in section \ref{sec:discussion-demographic}


\subsection{Value ratios}
\label{sec:discussion-value-ratios}
%
Regarding the value ratios, most results align with our hypotheses.
%
The resulting values of both the ME and the 100-level scale follow the same order ($V_{fn} < V_{fp} < V_{r} < V_{tp} < V_{tn}$).
%
We noticed that participants disagreed the most with scenarios FN3 and FN7.
%
According to the annotations given by \citet{basile2019semeval}, both scenarios are hate speech targeted at an individual and contain aggressive speech.
%
The results of FN3 and FN7 might suggest that participants are more likely to disagree with FN predictions for aggressive hate speech targeted at individuals.
%

%
As expected, we found that participants disagree with the FP, FN, and rejected predictions, that the value of an FN is lower than an FP, and that the average value of an FP and an FN is lower than the rejection value.
%
The results show that participants find a hateful post that is not detected worse than a non-hateful post detected as hate speech.
%
This finding is in line with our hypothesis that tolerating hate speech (FN predictions) harms social media users more than forbidding neutral speech (FP predictions).
%
The value of rejection is the closest to 0 (neutral) because, according to our formulations in the survey, rejected predictions only reduce the value by the human moderation effort and do not cause much benefit or harm since the human moderator needs to handle the prediction within 24 hours.
%

%
However, two things were somewhat surprising.
%
First, participants appreciated correct predictions more than incorrect predictions since participants gave higher absolute values to TP and TN scenarios than FP and FN scenarios.
%
We expect participants to give lower absolute response values to correct predictions since it is expected from the automatic detection algorithms to produce correct predictions.
%
However, we look at this from a computer science perspective, where we want to prevent incorrect predictions, whereas the participants might think producing correct predictions is more critical.
%
Second, we were surprised that the TN value was greater than TP, while we expected the opposite to hold.
%
One possible reason could be that people disagree more on what is considered hateful among the TP scenarios, as seen in figure \ref{fig:hatefulness}.
%
This observation is in line with the findings of \citet{ross2017measuring}, as the authors found low agreement among participants regarding labelling social media posts as hate speech.
%

\subsection{Reliability analysis}
\label{sec:discussion-reliability}
%
According to the Krippendorff's alpha values ($\alpha$), the results of the ME scale are reliable, indicating that the ME technique is suitable for estimating the value ratios.
%
Contrary to our hypothesis, the results indicated that the 100-level scale is less reliable than the ME scale.
%
We would expect many participants to give response values of -100, 100, or 0 as the 100-level scale is bounded, and, therefore, we would expect higher alpha values for the 100-level scale compared to the ME scale.
%

%
In general, the results also showed low alpha values when we computed it for each group of scenarios with the same type (TP, TN, FP, FN, and rejection).
%
Users tend to agree more on incorrect predictions than on correct predictions, indicating that participants agree more on the harm caused by incorrect predictions.
%
We can explain the low reliability values by looking at the calculation of Krippendorff's alpha.
%
In this calculation, we measure the difference between the expected difference and the observed difference.
%
When we consider the response values to all scenarios, the values tend to follow the same trend; positive values for correct predictions and negative values for incorrect and rejected predictions.
%
When we consider the response values to the scenarios of the same type, e.g. all TP scenarios, the values seem more randomly distributed as each participant uses a different positive response value to the TP scenarios.
%
Therefore, when considering all scenarios, the observed difference between the response values is closer to the difference expected by chance, resulting in higher alpha values.
%

% CONTINUE HERE
\subsection{Validity analysis}
\label{sec:discussion-validity}
The cross-modality validation between the ME and the 100-level scales showed that the response values to both scales are highly correlated, indicating that we validated the ME technique for measuring people's opinions about different hate speech detection scenarios.
%
The S-shaped curve in figure \ref{fig:correlation} is because for two reasons.
%
First, the magnitude estimates are skewed towards 0 because of the normalization procedure.
%
Second, the 100-level scores are skewed towards the upper and lower bounds of 100 and -100 as the participants are more likely to assign the highest or lowest value.

\subsection{Demographic analysis}
\label{sec:discussion-demographic}
We analyzed several demographic features (sex, student, continent, nationality, language, and ethnicity) to see if significant differences exist between groups of participants in the response values to all scenarios.
%

%
For all scenarios, we found no differences between men and women.
%
This finding is in line with the work of \citet{gold2018women}, as the authors did not find any differences between men and women and how they perceive hate.
%

%
For the remaining five features, we found significant differences between groups of participants for only a small number of scenarios.
%
Furthermore, for the scenarios and features with more than two groups (nationality, language, and ethnicity) where we found significant differences, we often did not find any significant pairwise differences between the groups.
%
These results indicate that for our dataset, people with different demographic characteristics tend to give the same judgements to different hate speech detection scenarios.
%
Nevertheless, the results show that people with different nationalities, languages, and ethnicities are more likely to differ in their opinions about hate speech detection scenarios than people of different sex or student status.
%

% The results of both the Kruskal-Wallis and the pairwise Mann-Whitney U tests are very similar between the nationality and language features.
% %
% This can be explained by the fact that the groups of both features are very similar as only a couple of participants differ between the two features.
% %

%
We found the most group differences for scenarios FP7 and REJ4 among all features, which is surprising since both scenarios include a neutral social media post.
%
It is unclear why FP7 had so many significant differences, as the post is neutral and not about any sensitive topic.
%
However, the social media post used in REJ4 is about refugees, which can be a politically sensitive topic.
%
People with different demographic characteristics, such as continent, language, nationality, or ethnicity, could have different opinions about this topic.
%
There were few pairwise group differences for both scenarios and the features of nationality and language.
%
However, we observed differences for two of the three pairwise combinations for the ethnicity feature and both scenarios.
%
Nevertheless, given these results, there is not enough evidence to explain why scenarios such as FP7 and REJ4 cause more group differences than other scenarios.
%

%
Also, we found that hateful social media posts are more likely to cause group differences than non-hateful posts, as we have more scenarios with group differences that contain hateful posts (10 in total) than non-hateful posts (5 in total).
%

%
We observed the most pairwise significant differences for scenario FN5.
%
Scenario FN5 contains a hateful social media post about building the wall across the border between the United States and Mexico.
%
There are five posts about building the wall, including hateful and non-hateful posts.
%
For four out of the five posts, we found at least one feature with significant differences between the groups of participants, suggesting that group differences depend on the topic of the social media post.
%


\section{Value-sensitive rejection}
\label{sec:discussion-rejection}
We analyze three aspects of our value-sensitive rejector.
%
First, we analyze how the rejector behaves when applied to different hate speech detection models and datasets (the \emph{seen} and \emph{unseen} datasets).
%
The \emph{seen} dataset is a test set sampled from the same dataset as the training set.
%
In contrast, the \emph{unseen} dataset is a test set sampled from a completely different dataset to simulate how the models perform on new and unfamiliar data.
%
Second, we study whether value-sensitive rejection of ML predictions can be beneficial for hate speech detection.
%
Finally, we compare our value-sensitive metric to machine metrics such as accuracy.
%

%
We observed that the three hate speech detection models are not well-calibrated, meaning many high-confident incorrect and low-confident correct predictions exist.
%
Therefore, when we apply a rejection threshold, we have the problem of accepting many incorrect predictions or rejecting many correct predictions.
%
Nevertheless, we observed that the models are more confident in the correct predictions than the incorrect predictions, making the value-sensitive rejector still useful.
%

%
The results of our value-sensitive metric were very similar for all three models and both datasets.
%
When we consider all value ratios, accepting all predictions seems the most valuable for both the \emph{seen} and \emph{unseen} data.
%
This result is not surprising as the absolute magnitudes of TP and TN are greater than the absolute magnitudes of FP and FN, and there are more TP and TN predictions than FP and FN predictions.
%
Therefore, the gains of accepting all correct predictions outweigh the costs of accepting all incorrect predictions for all models and datasets.
%

%
We believe it is more critical to focus on punishing incorrect predictions, as we want to minimize harm to social media users.
%
Therefore, in the second part of the experiments with the value-sensitive rejector, we no longer focused on rewarding correct predictions, implying $V_{tp}=0$ and $V_{tn}=0$.
%
As a result and according to the formulation of the value-sensitive metric (formula \ref{for:final-V}), accepted predictions increase the total value by the value of rejection ($V_r$) and correct predictions that are rejected decrease the total value by the value of rejection.
%
For the \emph{seen} data, the results of the optimal rejection threshold show that by not rewarding correct predictions, a significant fraction of the predictions can be accepted from all three models and a smaller fraction rejected.
%
All three models are valuable for the \emph{unseen} data, but very few predictions can be accepted, and the majority are rejected.
%
The results of the \emph{unseen} data also demonstrate that hate speech detection models are susceptible to bias, in line with the findings of related studies \citep{arango2019hate,grondahl2018all}.
%
When we accept all predictions, all three models are valuable for the \emph{seen} data but invaluable for the \emph{unseen} data, putting the viability of all models into question.
%
Therefore, the results show that our value-sensitive rejector can benefit hate speech detection and help us determine when to rely on the ML models.
%

%
Finally, we compared the results of our value-sensitive metric with machine metrics like accuracy.
%
If we accept all predictions, we find that both metrics indicate that the DistilBERT model performed the best.
%
However, when we consider not rewarding correct predictions for \emph{unseen} data, both metrics return different results.
%
According to our value-sensitive metric, the CNN model is the best, while accuracy indicates that either the LR or DistilBERT model is the best.
%
We think that the CNN model has a higher total value as it produces fewer FN predictions (which are costly) than the other two models.
%

%
We see some interesting things when we compare the value-sensitive metric for the optimal rejection thresholds with the accuracy metric.
%
For most configurations, both metrics return the same results, namely that the DistilBERT model is the best.
%
However, when considering not rewarding correct predictions for the unseen data, we see that the LR model performs the best and gets a slightly higher total value than the DistilBERT model for the optimal rejection threshold.
%
What makes this finding interesting is that while the accuracies of the original models are the same (64\%), we would expect that the DistilBERT has a higher total value because the DistilBERT model has a higher accuracy of the accepted predictions and a lower rejection rate.
%
One explanation might be that we found that the LR model achieves a higher total value since it rejects more FN predictions and accepts fewer FN predictions than the DistilBERT model for the optimal rejection threshold.
%

\section{Implications}
Related studies recognized machine metrics' shortcomings, such as accuracy \citep{casati2021value,olteanu2017limits,rottger2020hatecheck}.
%
%
While \citet{rottger2020hatecheck} focused on evaluating hate speech detection models by presenting a suite of automated tests, we focused on improving existing hate speech detection models by adopting a reject option.
%
\citet{olteanu2017limits} claim that we need more human-centred metrics that take the perceived cost of incorrect decisions into account instead of using abstract metrics such as precision.
%
They state that this perceived cost should depend on the context of the specific problem and the type of incorrect decisions \citep{olteanu2017limits}.
%
Our work aligns with theirs as we presented a value-sensitive metric that considers human value and conducted a survey study to retrieve the perceived value of social media users for TP, TN, FP, FN, and rejected predictions in hate speech detection.
%
%
\citet{casati2021value} promoted the use of value-sensitive metrics.
%
They recognized the limits of machine metrics such as accuracy, as two models could have the same accuracy, but one model could be more valuable than the other \citep{casati2021value}.
%
Our experiments demonstrated this difference as we found that the best model according to our value-sensitive metric could be different from the best model according to the accuracy metric.
%
We believe these findings can benefit industry and research, as many domains exist where tasks cannot be fully automated and where value-sensitive rejection can increase the utility of human-AI collaboration.
%

%
Regarding the survey study, we got several interesting findings.
%
Social media users appreciate correct hate speech predictions more than they detest incorrect predictions.
%
However, in terms of inter-rater reliability, they agree more on recognizing the harm caused by incorrect predictions than the gain from correct predictions.
%
Overall, the inter-rater reliability values of all scenarios of the same type were low.
%
This observation is in line with the findings of \citet{ross2017measuring}, where the authors also found low Krippendorff's alpha values when asking participants to annotate hate speech.
%
We did not find many significant differences between groups of different demographical characteristics.
%
Like \citet{gold2018women}, we did not find any differences between men and women and how they perceive hate.
%
However, we found more differences when looking at other demographical features, such as nationality, language, or ethnicity, implying that these features are more likely to cause group differences.
%

%
Regarding the hate speech detection models, we found that BERT models are indeed promising for hate speech detection, given the recent popularity of BERT models for NLP applications \citep{edwards2021best,alatawi2021detecting}.
%
The results with all three hate speech detection models also demonstrated the impact of dataset bias since we found significant performance drops in terms of both the value of our value-sensitive metric and the accuracy metric.
%
The experiments with the \emph{unseen} data resulted in lower total values and accuracies compared to the \emph{seen} data, indicating that hate speech datasets are biased.
%
Once we train hate speech detection models on such biased datasets, the models also become biased.
%
Our results fit the findings of previous studies where the authors found a significant performance drop when they trained models on one dataset and evaluated them on another \citep{grondahl2018all, arango2019hate}.
%

%
Regarding the methodology, we believe the ME technique is interesting for social science-related problems where the goal is to retrieve human-perceived value ratios.
%
We showed how we could use the value ratios in a value-sensitive metric that measures the total human-perceived value of ML models with a reject option.
%
We further demonstrated how we could create a human-machine co-creation solution for hate speech detection by using the value-sensitive metric to calculate the optimal rejection threshold for which the model achieves the highest total value.
%
We used the optimal threshold to determine when we could trust machine predictions and when we needed to pass machine predictions to a human moderator.

\section{Limitations}
\label{sec:discussion-limitations}
In this section, we list the limitations of the survey study and the value-sensitive rejector.
%

%
Regarding the survey study, we had a limited sample size of 68 participants per scale due to a constrained budget.
%
We expect more reliable results when experimenting with larger sample size.
%

%
Second, we limited the number of scenarios to eight per type, each including either a hateful or non-hateful post depending on the scenario type.
%
We expect the results to be more reliable if the experiment had been repeated several times with additional sets of social media posts for multiple groups of participants.
%
Nevertheless, we believe the results are still reliable since we performed a content analysis procedure for selecting the most representative social media posts for our experiment.
%

%
Third, dealing with hateful content on social media platforms remains controversial, even for governmental institutions and social media companies.
%
We believe the results will differ when we use different descriptions in the scenarios.
%
Initially, we explained in the pilot survey that SocialNet removes hateful posts.
%
After gathering the results, we noticed that participants assigned larger absolute values to the TP and TN scenarios than to the FP and FN scenarios.
%
Therefore, we decided to update the descriptions to rank posts lower instead of removing posts and explained that it is expected from detection systems to produce correct predictions and that incorrect predictions might cause harm to social media users.
%
However, after updating the descriptions, we did not notice any difference as participants still assigned larger absolute values to TP and TN scenarios.
%

%
Finally, we should point out the limitations of the demographic analysis.
%
We did not apply any demographical constraints when gathering participants for the survey study.
%
As a result, the demographical characteristics of the participants were entirely random, and the sample sizes were relatively small.
%
For example, most participants in our experiment lived in South Africa or Poland, and we only had five participants from Spain.
%
Although the sample sizes were large enough for the statistical tests, they were not large enough to represent the populations of entire countries.
%
We did not find enough evidence that people with different demographic characteristics have different opinions about hate speech detection scenarios.
%
However, if we repeated the experiment with larger sample sizes, we still expect to find group differences for some demographical features, such as nationality.
%
At the same time, we also believe that for the features where we did find significant differences between demographic groups, some of them might have happened by chance.
%
Either because participants did not understand the scenario, their lack of English, or because they rushed through the survey.
%

%
Regarding our value-sensitive rejector, we believe our approach has several limitations.
%

%
First, the rejection threshold is calculated empirically and depends highly on the choice of the test dataset.
%
As we have seen in our experiments, we retrieved different optimal rejection thresholds for different test datasets (the \emph{seen} and \emph{unseen} datasets).
%
Factors such as \emph{sample retrieval} or \emph{sample annotation} bias (refer to section \ref{sec:related-work-challenges}) explain why we got different results for the \emph{seen} and \emph{unseen} datasets.
%
Therefore, when using the value-sensitive rejector, it is essential to use a test set that is as similar to real-world data as possible.
%

%
Second, we think using well-calibrated models in the value-sensitive rejector is best.
%
Although calibration methods such as temperature scaling can improve existing classification models, these techniques are limited as we still observed many high confident errors.
%


\section{Recommendations}
\label{sec:discussion-recommendations}
We believe that future research on social-science-related problems, such as hate speech detection, should focus more on creating human-AI collaboration solutions and that these solutions should take human value into account.
%
We noticed that most research in hate speech detection focuses solely on creating fully-automated classification models with accuracy as its optimization target.
%
We think solutions such as ML with rejection are promising to increase the utility of classification models for tasks that cannot be fully automated, such as hate speech detection.
%
Furthermore, we showed the limitations of machine metrics, such as accuracy, as they do not consider the context of the problem.
%
We found through our survey study that different types of machine errors have different costs according to the social media users.
%
Therefore, we think that future work should also focus more on developing value-sensitive or human-centred metrics.
%

%
Given the limitations of our survey study, we suggest repeating our experiment with larger sample sizes to increase the reliability of the results.
%
Also, we think it would be interesting to study which factors influence the user perception of hate speech detection scenarios.
%
We believe that user perception depends on many factors, such as the scenario's description, the post's topic, whether the post is offensive or not, and the post's target(s).
%
Finally, we think it would be interesting for future research to study the effects of demographical characteristics in more detail by repeating the experiment for different demographic groups with larger sample sizes.
%

%
Given the limitations of our value-sensitive rejector, it would be interesting to create a hybrid solution of our value-sensitive rejector and (un)known unknown detection techniques.
%
By (un)known unknowns, we mean the low confident correct and high confident incorrect predictions.
%
If the underlying classification model of our value-sensitive rejector is not well-calibrated, then we end up with many (un)known unknowns.
%
We suggest that future work combine (un)known unknown detection techniques with our value-sensitive rejector so that less correct and more incorrect predictions are rejected and that more correct and less incorrect predictions are accepted.
%
Finally, as the optimal rejection threshold is calculated empirically on a hate speech dataset, we should prevent ourselves from using biased datasets.
%
For example, the datasets we used for the experiments collected their data using specific keywords and annotated the data samples with only three annotators.
%
Therefore, we suggest preventing sample retrieval and sample annotation bias as much as possible by collecting data samples that are the most representative and annotating them using a large group of annotators with diverse demographic characteristics.