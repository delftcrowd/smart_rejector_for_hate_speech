\chapter{Discussion}
The main goal of this project was to propose a way of rejecting ML model predictions in a value-sensitive manner for hate speech detection.
%
We split this up into two parts.
%
First, we wanted to determine how we could measure the total value of ML models with a reject option.
%
We proposed a value-sensitive metric that measures the total value of an ML model for some rejection threshold, where we reject and accept all predictions with a confidence value below and above the threshold, respectively.
%
This calculation is based on a set of predictions for some datasets and the value ratios between TP, TN, FP, FN, and rejected predictions.
%
By maximizing the total value, we can find the optimal rejection threshold.
%
Second, we wanted to develop a method for determining these value ratios necessary for the metric's calculations.
%
We proposed to estimate the value ratios in a large survey study using the ME scale.
%
The results from chapter \ref{ch:results} showed several key findings:
\begin{itemize}
    \item The survey's results indicated that the ME technique is suitable for retrieving the value ratios as the results passed both the reliability and validity analyses.
    \item We did not observe significant statistical differences for most scenarios between groups of participants with different demographical characteristics.
    \item The experiments with the \emph{seen} data showed that our value-sensitive rejector could be beneficial for detecting hate speech when we consider not rewarding correct predictions.
    \item The experiments with the \emph{unseen} data demonstrated that hate speech detection models are susceptible to bias, affecting the results of our value-sensitive rejector since we had to reject more predictions when we considered not rewarding correct predictions.
\end{itemize}
%

%
This chapter analyzes the results from chapter \ref{ch:results} in greater detail.
%
First, we discuss the main findings of the survey study in section \ref{sec:discussion-survey} and our value-sensitive rejector in section \ref{sec:discussion-rejection}.
%
Finally, we highlight some limitations of our approach in section \ref{sec:discussion-limitations} and give some recommendations in section \ref{sec:discussion-recommendations}.


\section{Survey study}
\label{sec:discussion-survey}
%
In each scenario, we first asked the participant to indicate if they thought the social media post was hateful or not.
%
We found three scenarios for which more than 40\% disagreed with the ground truth label (FN5, REJ5, and REJ7).
%
We also recognized this in the ME response values since participants generally were neutral about these scenarios.
%
Scenarios FN5 and REJ5 are annotated as non-aggressive hate speech targeted at a generic group and seem less hateful than the other posts, and the post in REJ7 contains an offensive slur.
%
Given the nature of these social media posts used in these scenarios, it might explain the larger disagreement about annotating it as hateful/non-hateful between participants for these scenarios.
%

%
We simulated the TP, TN, FP, FN, or rejection scenarios by asking the second and third questions where the participant had to provide a response value using either the ME or the 100-level scale to express their agreement or disagreement with SocialNet's decision.
%
We analyze the resulting response values by looking at three aspects.
%
First, we analyze the value ratios from the survey that uses the ME scale in section \ref{sec:discussion-value-ratios}.
%
Then we discuss whether the ME technique passes the reliability and validity analyses in sections \ref{sec:discussion-reliability} and \ref{sec:discussion-validity}.
%
Finally, we analyze the results of the demographic analysis in section \ref{sec:discussion-demographic}


\subsection{Value ratios}
\label{sec:discussion-value-ratios}
%
Regarding the value ratios, most results are in line with our hypotheses.
%
The resulting values of both the ME and the 100-level scale follow the same order ($V_{fn} < V_{fp} < V_{r} < V_{tp} < V_{tn}$).
%
We noticed that participants disagreed the most with scenarios FN3 and FN7.
%
According to the annotations given by \citet{basile2019semeval}, both scenarios are hate speech targeted at an individual and contain aggressive speech.
%
The results of FN3 and FN7 might suggest that participants disagree the most with FN predictions for aggressive hate speech target to individuals.
%
As expected, we found that participants disagree with the FP, FN, and rejected predictions, that the value of an FN is lower than an FP, and that the average value of an FP and an FN is lower than the rejection value.
%
The results show that participants find a hateful post that is not detected worse than a non-hateful post detected as hate speech.
%
This finding is in line with our hypothesis that tolerating hate speech (FN predictions) negatively impacts social media users more than forbidding neutral speech (FP predictions).
%
The value of rejection is the closest to 0 (neutral) because rejected predictions do not cause any benefit or harm but only reduce the value by the human moderation effort.
%
However, two things were somewhat surprising.
%
First, participants appreciated correct predictions more than incorrect predictions since participants gave higher absolute values to TP and TN scenarios than FP and FN scenarios.
%
We expect participants to give lower absolute response values to correct predictions since it is expected from the automatic detection algorithms to produce correct predictions.
%
However, we look at this from a computer science perspective, where we want to prevent incorrect predictions, whereas the participants might think producing correct predictions is more critical.
%
Second, we were surprised that the TN value was greater than TP, while we expected the opposite to hold.
%
One possible reason could be that people disagree more on what is considered hateful among the TP scenarios, as seen in figure \ref{fig:hatefulness}.
%
This observation is in line with the findings of \citet{ross2017measuring}, as the authors found low agreement among participants regarding labelling social media posts as hate speech.
%

\subsection{Reliability analysis}
\label{sec:discussion-reliability}
%
According to the Krippendorff's alpha values ($\alpha$), the results of the ME scale are reliable, indicating that the ME technique is suitable for estimating the value ratios.
%
Contrary to our hypothesis, the results indicated that the 100-level scale is less reliable than the ME scale.
%
We would expect many participants to give response values of -100, 100, or 0 as the 100-level scale is bounded, and, therefore, we would expect higher alpha values for the 100-level scale compared to the ME scale.
%
The results also showed low alpha values when we computed it for each group of scenarios with the same type (TP, TN, FP, FN, and rejection).
%
Users tend to agree more on incorrect predictions than on the correct predictions, indicating that participants disagree more on what constitutes hate speech in the first place.
%
This phenomenon is in line with the findings of \citet{ross2017measuring}, where the authors also found low Krippendorff's alpha values when asking participants to annotate hate speech.
%
The alpha values for the grouped scenarios by type are lower compared to when we consider all scenarios.
%
We can explain this phenomenon by looking at the calculation of Krippendorff's alpha.
%
In this calculation, we measure the difference between the expected difference and the observed difference.
%
When we consider the response values to all scenarios, the values tend to follow the same trend; positive values to correct predictions and negative values for incorrect and rejected predictions.
%
When we consider the response values to the scenarios of the same type, e.g. all TP scenarios, the values seem more randomly distributed as each participant uses a different positive response value to the TP scenarios.
%
Therefore, when considering all scenarios, the observed difference between the response values is closer to the difference expected by chance, resulting in higher alpha values.
%

%
\subsection{Validity analysis}
\label{sec:discussion-validity}
The cross-modality validation between the ME and the 100-level scales showed that the response values to both scales are highly correlated, indicating that we validated the ME technique for measuring people's opinions about different hate speech detection scenarios.
%
The S-shaped curve in figure \ref{fig:correlation} is because for two reasons.
%
First, the magnitude estimates are skewed towards 0 because of the normalization procedure.
%
Second, the 100-level scores are skewed towards the upper and lower bounds of 100 and -100 as the participants are more likely to assign the highest or lowest value.

\subsection{Demographic analysis}
\label{sec:discussion-demographic}
We analyzed several demographic features (sex, student, continent, nationality, language, and ethnicity) to see if significant differences exist between groups of participants in the response values to all scenarios.
%

%
For all scenarios, we found no differences between men and women.
%
This finding is in line with the work of \citet{gold2018women}, as the authors did not find any differences between men and women and how they perceive hate.
%

%
For the remaining five features, we found significant differences between groups of participants for only a small number of scenarios.
%
Furthermore, for the scenarios and features with more than two groups (nationality, language, and ethnicity) where we found significant differences, we often did not find any significant pairwise differences between the groups.
%
These results indicate that for our dataset, people with different demographic characteristics tend to give the same judgements to different hate speech detection scenarios.
%
Nevertheless, the results show that people with different nationalities, languages, and ethnicities are more likely to differ in their opinions about hate speech detection scenarios than people of different sex or student status.
%

% The results of both the Kruskal-Wallis and the pairwise Mann-Whitney U tests are very similar between the nationality and language features.
% %
% This can be explained by the fact that the groups of both features are very similar as only a couple of participants differ between the two features.
% %

%
We found the most group differences for scenarios FP7 and REJ4 among all features, which is surprising since both scenarios include a neutral social media post.
%
It is unclear why FP7 had so many significant differences as the post is neutral and not about any sensitive topic.
%
However, the social media post used in REJ4 is about refugees, which can be a politically sensitive topic.
%
People with different demographic characteristics, such as continent, language, nationality, or ethnicity, could have different opinions about this topic.
%
There were few pairwise group differences for both scenarios and the features of nationality and language.
%
However, we observed differences for two of the three pairwise combinations for the ethnicity feature and both scenarios.
%
Nevertheless, given these results, there is not enough evidence to explain why scenarios such as FP7 and REJ4 cause more group differences than other scenarios.
%

%
Also, we found that hateful social media posts are more likely to cause group differences than non-hateful posts, as we have more scenarios with group differences that contain hateful posts (10 in total) than non-hateful posts (5 in total).
%

%
We observed the most pairwise significant differences for scenario FN5.
%
Scenario FN5 contains a hateful social media post about building the wall across the border between the United States and Mexico.
%
There are five posts about building the wall, including hateful and non-hateful posts.
%
For four out of the five posts, we found at least one feature with significant differences between the groups of participants, suggesting that group differences depend on the topic of the social media post.
%


\section{Value-sensitive rejection}
\label{sec:discussion-rejection}
- We observed that most models are not well callibrated, meaning that there are many high confident incorrect predictions and low confident correct predictions. This makes it hard to find apply a rejection threshold since we then accept many incorrect predictions and reject many correct predictions. Nevertheless, we still observed that the models are more confident in the correct predictions than the incorrect predictions. Therefore, the value-sensitive rejector is still useful for finding the optimal rejection threshold for which we achieve the maximum total value of accepting correct predictions and rejecting incorrect predictions.
- When we consider all value ratios, then accepting all predictions seems the most valuable according to the results of the value-sensitive rejector. This result is not surprising as the absolute values of TP and TN ($V_{tp}$ and $V_{tn}$) are greater than the absolute values of the FP and FN ($V_{fp}$ and $V_{fn}$) and there are more TP and TN predictions than FP and FN predictions. Therefore, the gains of accepting all correct predictions outweights the costs of accepting all incorrect predictions for all models and datasets.
- We believe it is more critical to focus on punishing incorrect predictions, as we want to minimize harm to the social media users. Therefore, In the second part of the experiments with the value-sensitive rejector, we decided to no longer focus on rewarding correct predictions, implying that $V_{tp}=0$ and $V_{tn}=0$. As a result and according to the formulation of the value-sensitive metric (formula \ref{for:final-V}), correct predictions that are accepted increase the total value by the value of a rejection ($V_r$) and correct predictions that are rejected decrease the total value by the value of a rejection.


% Continue here...
% The result in figure 3a shows a steady increase in value before it peaks around 0.8 for each of the three models, eventually falling again and becoming negative as almost all predictions are rejected. Hence, there is a strong incentive to reject some (but not all) predictions for the seen data. At the points where values are maximized, we found an optimal balance between accepting and rejecting predictions. Figure 3b shows that the values continually rise for all three models, only peaking as the rejection threshold approaches 1. This indicates that the model is very uncertain regarding its predictions for the unseen data, which may be expected. Initially at the 0.5 rejection threshold, the value is negative as all predictions are accepted. When the rejection threshold increases, the value rises steadily since too many incorrect predictions are made. This indicates that the model is not performing well at the task (e.g., many high confidence FP and FN), and thus the optimal condition to reject most predictions puts the model’s viability into question. In sum, the results show that by penalizing incorrect predictions without rewarding correct predictions, a significant fraction of the predictions can be accepted from all three models. For unseen data, however, very few predictions from these models can be accepted and the majority are rejected. Such a result confirms the bias in the dataset as also found in previous studies (Arango, P ́ erez, and Poblete 2019; Gr  ̈ ondahl et al. 2018). The results also show the utility of value as a metric in guiding the decision on when to reject machine predictions. Value utility is further confirmed in the results in table 3 from our experiment on optimal model selection: the best model selected by value is different compared to using accuracy as the metric.
\section{Implications}
\todo[inline]{Explain that \cite{olteanu2017limits} claims that we need more human-centred metrics instead of abstract metrics such as precision and we agree with that by introducing our own human-centred metric}


\todo[inline]{Answer research questions}

\section{Limitations}
\label{sec:discussion-limitations}
\todo[inline]{Hate speech is difficult domain as there tend to be a lot of disagreement between people about what is considered hate speech and what not. \citet{ross2017measuring} found low Krippendorff alpha values in a hate speech survey. So our findings are in line with theirs.}
\todo[inline]{Explain limitations of the metric and the survey study}
\todo[inline]{Finally, we should point out the limitations of the demographic analysis.}
\todo[inline]{First, the sample sizes of the demographic groups are not representative of the population of those groups.}
\todo[inline]{For example, for the nationality feature, we had five participants from Spain.}
\todo[inline]{Second, we have to keep in mind that some features where we did find sig. differences might have happened by chance. There may have been participants that did not understood the scenario, either because the lack of english or because they rushed through the survey.}
\todo[inline]{We represented the scenarios by explaining that we would rank social media posts lower higher on people's feeds. Initially, we explained in the pilot survey that we would remove or keep posts. However, we noticed larger disagreement values in the FP and TP cases since apparantly some might think that we should allow everything on the web even hateful posts. Therefore, we are aware that the decisions of social media platforms on how to deal with detected hateful posts impacts people's opinions about it.}
\todo[inline]{The rejection threshold is calculated using the test set. This test set needs to be as realistic as possible. Furthermore we need to have calibrated models since we rely purely on the confidence values. This is also hard to realize. Temperature scaling can help, but it is still limited.}
\todo[inline]{Also, we should keep in mind that because of the clustering analysis, we created a selection of social media posts as diverse as possible. So some posts may not result in any differences between groups while others do.}

\section{Recommendations}
\label{sec:discussion-recommendations}
\todo[inline]{Magnitude Estimation seems promising for future research in HCI.}
\todo[inline]{Personal and demographic characterisitcs might have  a big impact. So further analysis on those aspects seem relevant.}
\todo[inline]{Perhaps we can train ML models using the values of TP, TN, FP, FN, rejection in an integrated rejector. So we train the ML model and the rejector simultanously using the values from the survey. So then during training, the FN predictions are punished more than FP predictions.}