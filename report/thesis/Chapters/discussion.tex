\chapter{Discussion}
The main goal of this project was to propose a way of rejecting ML model predictions in a value-sensitive manner for hate speech detection.
%
We split this up into two parts.
%
First, we wanted to determine how we could measure the total value of ML models with a reject option.
%
We proposed a value-sensitive metric that measures the total value of an ML model for some rejection threshold, where we reject and accept all predictions with a confidence value below and above the threshold, respectively.
%
This calculation is based on a set of predictions for some datasets and the value ratios between TP, TN, FP, FN, and rejected predictions.
%
By maximizing the total value, we can find the optimal rejection threshold.
%
Second, we wanted to develop a method for determining these value ratios necessary for the metric's calculations.
%
We proposed to estimate the value ratios in a large survey study using the ME scale.
%
The results from chapter \ref{ch:results} showed several key findings:
\begin{enumerate}
    \item The results indicated that the ME technique is suitable for retrieving the value ratios as the results passed both the reliability and validity analyses.
    \item We did not observe significant statistical differences between groups of participants for most scenarios and demographical features.
    \item The results showed that our value-sensitive rejector could be beneficial for detecting hate speech when we consider punishing incorrect predictions only.
    \item The experiments with the unseen data demonstrated that hate speech detection models are susceptible to bias, affecting the results of our value-sensitive rejector since we had to reject more predictions when we considered punishing incorrect predictions only.
\end{enumerate}
%

%
This chapter analyzes the results from chapter \ref{ch:results} in greater detail.
%
First, we discuss the main findings of the survey study in section \ref{sec:discussion-survey} and our value-sensitive rejector in section \ref{sec:discussion-rejection}.
%
Finally, we highlight some limitations of our approach in section \ref{sec:discussion-limitations} and give some recommendations in section \ref{sec:discussion-recommendations}.


\section{Survey study}
\label{sec:discussion-survey}
Regarding value ratios, most results were in line with our hypotheses and both scales returned the same results in terms of the order of the values ($V_{fn} < V_{fp} < V_{r} < V_{tp} < V_{tn}$).
%
As expected, we found that participants disagree with the FP, FN, and rejected predictions, the value of an FN is lower than an FP, and that the average value of an FP and an FN was lower than the rejection value.
%
The results show that participants find a hateful post not detected as hate speech worse than a non-hateful post detected as hate speech.
%
This is in line with our assumption of allowing hate speech (FN) to have a more negative impact on the social media users than suppressing freedom of speech by forbidding neutral speech (FP).
%
The value of rejection was the closest to 0 (neutral), because rejected predictions do not cause any benefit or harm but only reduce the value by the human moderation effort.
%
However, there are two things that are somewhat supprising.
%
First, participants appreciated correct predictions more than incorrect predictions since participants gave higher absolute values to TP and TN than FP and FN.
%
We expect participants to give lower absolute response values to correct predictions since it is expected from the automatic detection algorithms to produce correct predictions.
%
However, we look at this from a computer science perspective where we want to prevent incorrect predictions whereas the participants might think that correct predictions are more important.
%
Second, we are suprised to see that the value of a TN is greater than of a TP while we expected the opposite to hold.
%
One possible reason could be that people slightly disagree more on what is considered hateful among the TP scenarios, as can be seen in figure \ref{fig:hatefulness}.
%

%
% TODO: reliability

\todo[inline]{Discuss value ratios}
\todo[inline]{Discuss reliability values and why the values are low when looking at the scenario types only}
\todo[inline]{Discuss why TN3 and TN7 have highest agreement scores}
\todo[inline]{Discuss why FN3 and FN7 have highest disagreement scores}
\todo[inline]{Discuss why FP7 and REJ4 have most sig. differences among all features. And why TP6, FN5, and REJ1 have the second most differences. Not clear why FP7. But REJ4 is a neutral tweet about a political topic. People could have different opinions about refugees espcially from different nationalities. This might explain the differences.}
\todo[inline]{Only FN5 has the most pairwise sig. differences, which is also a tweet about the building the wall.}
\todo[inline]{However, when looking at the other scenarios, we mainly see that there are no differences between groups with different demographic features.}
\todo[inline]{For the sex feature, we do not find any differences between the men and women. This is what we expected as related work also found this (refer to work).}
\todo[inline]{Furthermore, for the features with multiple groups (more than two) where we did find sig. differences, we often do not find any pairwise sig. differences between the groups.}
\todo[inline]{Therefore, we can conclude that for our dataset, people with different demographic characterisitcs overall tend to give the same judgements.}
\todo[inline]{However, we can see that differences between groups of participants for features such as nationality, language, or ethnicity are greater than features such as sex or student.}
\todo[inline]{The results of both the Kruskal-Wallis and the pairwise Mann-Whitney U tests are very similar between the nationality and language features. This can be explained by the fact that the groups of both features are very similar as only a couple of participants differ between the two features.}
\todo[inline]{Also, we see that there are more hateful posts (10) than non-hateful ones (5) with significant differences, meaning that hateful tweets are more sensitive to trigger differences between groups with different demographic characterisitcs.}
\todo[inline]{Also, we should keep in mind that because of the clustering analysis, we created a selection of tweets as diverse as possible. So some posts may not result in any differences between groups while others do. For example, there are five posts, both hateful and non-hateful, about building a wall in America across the Mexican border and four posts where there is at least one feature with significant differences between the groups.}
\todo[inline]{Therefore, we can conclude that the significant differences between groups for a scenario depends highly on the content of the social media post.}

\section{Value-sensitive rejection}
\label{sec:discussion-rejection}

\section{Implications}
\todo[inline]{Explain that \cite{olteanu2017limits} claims that we need more human-centred metrics instead of abstract metrics such as precision and we agree with that by introducing our own human-centred metric}


\todo[inline]{Answer research questions}

\section{Limitations}
\label{sec:discussion-limitations}
\todo[inline]{Hate speech is difficult domain as there tend to be a lot of disagreement between people about what is considered hate speech and what not. \citet{ross2017measuring} found low Krippendorff alpha values in a hate speech survey. So our findings are in line with theirs.}
\todo[inline]{Explain limitations of the metric and the survey study}
\todo[inline]{Finally, we should point out the limitations of the demographic analysis.}
\todo[inline]{First, the sample sizes of the demographic groups are not representative of the population of those groups.}
\todo[inline]{For example, for the nationality feature, we had five participants from Spain.}
\todo[inline]{Second, we have to keep in mind that some features where we did find sig. differences might have happened by chance. There may have been participants that did not understood the scenario, either because the lack of english or because they rushed through the survey.}

\todo[inline]{The rejection threshold is calculated using the test set. This test set needs to be as realistic as possible. Furthermore we need to have calibrated models since we rely purely on the confidence values. This is also hard to realize. Temperature scaling can help, but it is still limited.}

\section{Recommendations}
\label{sec:discussion-recommendations}
\todo[inline]{Magnitude Estimation seems promising for future research in HCI.}
\todo[inline]{Personal and demographic characterisitcs might have  a big impact. So further analysis on those aspects seem relevant.}
\todo[inline]{Perhaps we can train ML models using the values of TP, TN, FP, FN, rejection in an integrated rejector. So we train the ML model and the rejector simultanously using the values from the survey. So then during training, the FN predictions are punished more than FP predictions.}