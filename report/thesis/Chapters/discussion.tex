\chapter{Discussion}
In this thesis project, we worked on a hybrid human-AI solution for detecting hate speech.
%
The main problem is that manual moderation is the most reliable but infeasible, and that automatic moderation through detection algorithms is the most performant but sometimes unreliable.
%
We focused on rejecting predictions of ML models for hate speech detection.
%
However, determing when to accept or reject predictions depends on the context and, more specifically, the implications of accepting/rejecting correct or incorrect predictions.
%
We denoted these implications as values of TP, TN, FP, FN, and rejected predictions.
%
Our main goal was finding out how we can reject ML predictions in a value-sensitive manner for hate speech detection.
%
We split this up into two parts.
%
First, we wanted to find out how we can measure the total value of ML models with a reject option.
%
By maximizing the total value, we know when we need to accept or reject predictions.
%
We tackled this by introducing a value-sensitive metric that we use for calculating the optimal rejection threshold.
%
Second, we wanted to determine the value ratios between TP, TN, FP, FN, and rejected predictions.
%
We determined these value ratios by conducting a large survey study in which we presented participants with different hate speech detection scenarios in which they had to provide their judgements using the ME scale.
%

%
This chapter analyzes the results from chapter \ref{ch:results}.
%
First, we discuss the main findings of the survey study in section \ref{sec:discussion-survey} and the main findings of our value-sensitive rejector in section \ref{sec:discussion-rejection}.
%
Then, we answer the our research questions in section \ref{sec:discussion-research}.
%
Finally, we highlight some limitations of our approach in section \ref{sec:discussion-limitations} and give some recommendations in section \ref{sec:discussion-recommendations}.


\section{Survey study}
\label{sec:discussion-survey}
\todo[inline]{Discuss value ratios}
\todo[inline]{Discuss reliability values and why the values are low when looking at the scenario types only}
\todo[inline]{Discuss why TN3 and TN7 have highest agreement scores}
\todo[inline]{Discuss why FN3 and FN7 have highest disagreement scores}
\todo[inline]{Discuss why FP7 and REJ4 have most sig. differences among all features. And why TP6, FN5, and REJ1 have the second most differences. Not clear why FP7. But REJ4 is a neutral tweet about a political topic. People could have different opinions about refugees espcially from different nationalities. This might explain the differences.}
\todo[inline]{Only FN5 has the most pairwise sig. differences, which is also a tweet about the building the wall.}
\todo[inline]{However, when looking at the other scenarios, we mainly see that there are no differences between groups with different demographic features.}
\todo[inline]{For the sex feature, we do not find any differences between the men and women. This is what we expected as related work also found this (refer to work).}
\todo[inline]{Furthermore, for the features with multiple groups (more than two) where we did find sig. differences, we often do not find any pairwise sig. differences between the groups.}
\todo[inline]{Therefore, we can conclude that for our dataset, people with different demographic characterisitcs overall tend to give the same judgements.}
\todo[inline]{However, we can see that differences between groups of participants for features such as nationality, language, or ethnicity are greater than features such as sex or student.}
\todo[inline]{The results of both the Kruskal-Wallis and the pairwise Mann-Whitney U tests are very similar between the nationality and language features. This can be explained by the fact that the groups of both features are very similar as only a couple of participants differ between the two features.}
\todo[inline]{Also, we see that there are more hateful posts (10) than non-hateful ones (5) with significant differences, meaning that hateful tweets are more sensitive to trigger differences between groups with different demographic characterisitcs.}
\todo[inline]{Also, we should keep in mind that because of the clustering analysis, we created a selection of tweets as diverse as possible. So some posts may not result in any differences between groups while others do. For example, there are five posts, both hateful and non-hateful, about building a wall in America across the Mexican border and four posts where there is at least one feature with significant differences between the groups.}
\todo[inline]{Therefore, we can conclude that the significant differences between groups for a scenario depends highly on the content of the social media post.}

\section{Value-sensitive rejection}
\label{sec:discussion-rejection}
\section{Implications}
\todo[inline]{Explain that \cite{olteanu2017limits} claims that we need more human-centred metrics instead of abstract metrics such as precision and we agree with that by introducing our own human-centred metric}

\section{Research questions}
\label{sec:discussion-research}

\todo[inline]{Answer research questions}

\section{Limitations}
\todo[inline]{Hate speech is difficult domain as there tend to be a lot of disagreement between people about what is considered hate speech and what not. \citet{ross2017measuring} found low Krippendorff alpha values in a hate speech survey. So our findings are in line with theirs.}
\todo[inline]{Explain limitations of the metric and the survey study}
\todo[inline]{Finally, we should point out the limitations of the demographic analysis.}
\todo[inline]{First, the sample sizes of the demographic groups are not representative of the population of those groups.}
\todo[inline]{For example, for the nationality feature, we had five participants from Spain.}
\todo[inline]{Second, we have to keep in mind that some features where we did find sig. differences might have happened by chance. There may have been participants that did not understood the scenario, either because the lack of english or because they rushed through the survey.}

\todo[inline]{The rejection threshold is calculated using the test set. This test set needs to be as realistic as possible. Furthermore we need to have calibrated models since we rely purely on the confidence values. This is also hard to realize. Temperature scaling can help, but it is still limited.}

\section{Recommendations}
\todo[inline]{Magnitude Estimation seems promising for future research in HCI.}
\todo[inline]{Personal and demographic characterisitcs might have  a big impact. So further analysis on those aspects seem relevant.}
\todo[inline]{Perhaps we can train ML models using the values of TP, TN, FP, FN, rejection in an integrated rejector. So we train the ML model and the rejector simultanously using the values from the survey. So then during training, the FN predictions are punished more than FP predictions.}