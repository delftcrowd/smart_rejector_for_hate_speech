\chapter{Value-sensitive rejection}
As concluded in the \nameref{sec:related-work}, there is a need for \textit{value-sensitive} metrics for measuring the performance of ML models, especially for social-technical applications such as hate speech detection.
%
We also concluded that manual human moderation is the most effective and that most automatic hate speech detection methods do not perform well on unseen data.
%
Therefore, in this project, we focus on rejecting ML predictions in a value-sensitive manner.
%
We do this by taking the values of TP, TN, FP, FN, and rejected predictions into account.
%
% Correct predictions (TP and TN) result in positive value (gains), while incorrect (FP and FN) and rejected predictions result in negative value (costs).
%
Section \ref{sec:survey} will explain how we assess these values.
%
For the remaining part of this chapter, we assume that we know these values.
%

%
In this chapter, we explain how we create a dependent rejector by introducing a confidence metric that measures the total value of an ML model with a reject option.
%
In \ref{sec:value-metric}, we explain how we construct the confidence metric, and in \ref{sec:state-of-the-art}, we discuss how we apply the metric to some of the state-of-the-art hate speech classification models.

\section{Value metric}
\label{sec:value-metric}
The idea of a rejecting ML predictions using a treshold is that for some treshold value $\tau$ in the range $[0, 1]$, we accept all predictions with confidence values that are greater than or equal to $\tau$ and reject all predictions with confidence values below $\tau$.
%
We use a confidence metric to find the optimal rejection threshold.
%
Here, we introduce our confidence metric as the value function $V(\tau)$ that measures the total value of some ML model with rejection threshold $\tau$.
%
We can determine the optimal rejection threshold by finding the $\tau$ value for which $V(\tau)$ is maximum.
%
The value of $V(\tau)$ depends on the values of TP, TN, FP, FN, and rejected predictions and is calculated for a set of predictions with their corresponding confidence values and actual labels.
%
We denote the values of TP, TN, FP, FN, and rejected predictions as $V_{tp}$, $V_{tn}$, $V_{fp}$, $V_{fn}$, and $V_r$ respectively.
%
From the set of predictions we can derive the subsets of TP, TN, FP, and FN predictions.
%
Note that we consider that $V_{tp}$ and $V_{tn}$ can be expressed as gains and, therefore, non-negative values and that $V_{fp}$, $V_{fn}$, and $V_{r}$ can be expressed costs and, therefore, non-positive values.
%
There is one condition that needs to hold: the reduction of total value by means of a rejection should always be smaller in magnitude than the value reduction of an incorrect prediction.
%
Otherwise adopting the reject option serves no purpose.
%
This condition can be formulated as:
% 
\begin{align}
    \label{for:value-condition}
    \frac{V_{FP} + V_{FN}}{2} < V_R,
\end{align}
%
For each $\tau$ value in $[0, 1]$, we would like to know whether the model with the reject option is more effective (increased $V(\tau)$) or less effective (decreased $V(\tau_)$ value).
%
In general, the metric should take the following conditions into account:
% 
\begin{enumerate}
    \item Correct accepted predictions should increase the value of $V(\tau)$, while incorrect accepted predictions should decrease the value of $V(\tau)$.
    \item Correct rejected predictions should decrease the value of $V(\tau)$, while incorrect rejected predictions should increase the value of $V(\tau)$.
\end{enumerate}
% 
We can convert these conditions into the following equations:
\begin{subequations}
    \label{for:conditions}
    \begin{align}
        \frac{\partial V}{\partial R_{tp}(\tau)} + \frac{\partial V}{\partial R_{tn}(\tau)} > 0, &  &
        \frac{\partial V}{\partial R^r_{tp}(\tau)} + \frac{\partial V}{\partial R^r_{tn}(\tau)} < 0, \label{for:conditions-tp-tn} \\
        \frac{\partial V}{\partial R_{fp}(\tau)} + \frac{\partial V}{\partial R_{fn}(\tau)} < 0, &  &
        \frac{\partial V}{\partial R^r_{fp}(\tau)} + \frac{\partial V}{\partial R^r_{fn}(\tau)} > 0, \label{for:conditions-fp-fn}
    \end{align}
\end{subequations}
%
where $R_t(\tau)$ and $R_t^r(\tau)$ are the fractions of accepted and rejected predictions respectively based on the rejection threshold $\tau$ and where $t \in [tp, tn, fp, fn]$.
%
We create a linear $V(\tau)$ function and assume that the values $V_t$ are known constants.
%
Subsequently, we can formulate $V(\tau)$ as:
\begin{align}
    \label{for:V}
    V(\tau) = \sum_{p} (V_p - V_r)R_p(\tau) + \sum_{q} (V_r - V_q)R^r_{q}(\tau),
\end{align}
%
where $p, q \in [tp, tn, fp, fn]$.
%
Conditions \ref{for:conditions-tp-tn} are satisfied by default since we assume that $V_{tp}$ and $V_{tn}$ are non-negative and $V_r$ is non-positive.
%
Conditions \ref{for:conditions-fp-fn} are satisfied under the condition of \ref{for:value-condition} and since we assume that $V_{fp}$, $V_{fn}$, and $V_r$ are negative.
% 
We can define the $R_t$ and the $R_t^r$ values by computing the integrals over the probability density functions (PDF) of the confidence values of the predictions with type $t$.
%
We can define $R_t$ by taking the integral over the interval $[\tau, 1]$, and $R_t^r$ by taking the integral over the interval $[0, \tau]$:
%
\begin{align}
    \label{for:R}
    \begin{aligned}
        R_{t}(\tau) = \int_\tau^1 D_{t}(x)dx & \quad & R_{t}^r(\tau) = \int_0^\tau D_{t}(x)dx
    \end{aligned}
\end{align}
%
By inserting the integrals from \ref{for:R} into \ref{for:V}, we get our final value function:
%
\begin{align}
    \label{for:final-V}
    V(\tau) = \sum_{t} (V_t - V_r)\int_\tau^1 D_{t}(x)dx + \sum_{t} (V_r - V_t)\int_0^\tau D_{t}(x)dx,
\end{align}
%
where $t \in [tp, tn, fp, fn]$ and $\tau$ is the rejection threhsold.
%
We can now use \ref{for:final-V} to calculate the total value of some ML model for all $\tau \in [0, 1]$ values.
%
The theoretical optimal rejection threshold is equal to the $\tau$ value for which we achieve the maximum value of $V(\tau)$.


\section{State-of-the-art}
\label{sec:state-of-the-art}
In this section, we will explain how we will apply the value-sensitive metric from section \ref{sec:value-metric} to some of the state-of-the-art automatic hate speech detection models so that we convert them into dependent rejectors.
%
In this experiment, we aim to find out three things.
%
First, we want to find out how the value-sensitive metric behaves on the different models and datasets.
%
Second, we want to know wheter rejecting ML predictions can be beneficial in the hate speech detection domain.
%
Finally, we will compare our value-sensitive metric to machine metrics such as accuracy and check whether they give different results.
%

\subsection{Models}
We will experiment with three different hate speech detection models based on the findings from related work in section \ref{sec:related-work-detection-algorithms}.
%
The first model is a traditional ML model.
%
We implement the Logistich Regression (LR) model with Character N-gram from \citet{waseem2016hateful} since this model achieved the best performance compared other traditional ML models \citet{davidson2017automated}.
%
The second model is DL model based on the work of \citet{agrawal2018deep} and \citet{badjatiya2017deep}.
%
We chose a Convolutional Neural Network (CNN) model since both studies found that it performs quite well.
%
Finally, our third model is a transformer model given its recent popularity in the NLP domain.
%
We selected the DistilBERT model since it's faster and smaller in size compared to BERT models while achieving similar performance \citep{sanh2019distilbert}.

\subsection{Calibration}
\todo[inline]{Explain what model calibration is and why it's necessary}

\subsection{Datasets}
\todo[inline]{Explain that we are going to use the SemEval and Waseem datasets and the concepts of seen and unseen data}

\subsection{Calculating the total value}
\todo[inline]{Explain how we are going to apply our metric to the trained state-of-the-art models to calculate the optimal rejection threshold given the values of TP, TN, FP, FN, and rejection (that we still need estimate)}
\todo[inline]{Explain that we use Kernel Density Estimation to calculate the PDF functions (or stick to prediction counts as in the paper).}
