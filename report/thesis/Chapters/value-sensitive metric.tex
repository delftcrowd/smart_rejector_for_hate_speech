\chapter{Value-sensitive rejection}
As concluded in the \nameref{sec:related-work}, there is a need for \textit{value-sensitive} metrics for measuring the performance of ML models, especially for social-technical applications such as hate speech detection.
%
We also concluded that manual human moderation is the most effective and that most automatic hate speech detection methods do not perform well on unseen data.
%
Therefore, in this project, we focus on rejecting ML predictions in a value-sensitive manner.
%
We do this by taking the values of TP, TN, FP, FN, and rejected predictions into account.
%
% Correct predictions (TP and TN) result in positive value (gains), while incorrect (FP and FN) and rejected predictions result in negative value (costs).
%
Section \ref{sec:survey} will explain how we assess these values.
%
For the remaining part of this chapter, we assume that we know these values.
%

%
In this chapter, we explain how we create a dependent rejector by introducing a confidence metric that measures the total value of an ML model with a reject option.
%
In \ref{sec:value-metric}, we explain how we construct the confidence metric, and in \ref{sec:state-of-the-art}, we discuss how we apply the metric to some of the state-of-the-art hate speech classification models.

\section{Value metric}
\label{sec:value-metric}
The idea of a rejecting ML predictions using a treshold is that for some treshold value $\tau$ in the range $[0, 1]$, we accept all predictions with confidence values that are greater than or equal to $\tau$ and reject all predictions with confidence values below $\tau$.
%
We use a confidence metric to find the optimal rejection threshold.
%
Here, we introduce our confidence metric as the value function $V(\tau)$ that measures the total value of some ML model with rejection threshold $\tau$.
%
We can determine the optimal rejection threshold by finding the $\tau$ value for which $V(\tau)$ is maximum.
%
The value of $V(\tau)$ depends on the values of TP, TN, FP, FN, and rejected predictions and is calculated for a set of predictions with their corresponding confidence values and actual labels.
%
We denote the values of TP, TN, FP, FN, and rejected predictions as $V_{tp}$, $V_{tn}$, $V_{fp}$, $V_{fn}$, and $V_r$ respectively.
%
From the set of predictions we can derive the subsets of TP, TN, FP, and FN predictions.
%
Note that we consider that $V_{tp}$ and $V_{tn}$ can be expressed as gains and, therefore, non-negative values and that $V_{fp}$, $V_{fn}$, and $V_{r}$ can be expressed costs and, therefore, non-positive values.
%
There is one condition that needs to hold: the reduction of total value by means of a rejection should always be smaller in magnitude than the value reduction of an incorrect prediction.
%
Otherwise adopting the reject option serves no purpose.
%
This condition can be formulated as:
% 
\begin{align}
    \label{for:value-condition}
    \frac{V_{FP} + V_{FN}}{2} < V_R,
\end{align}
%
For each $\tau$ value in $[0, 1]$, we would like to know whether the model with the reject option is more effective (increased $V(\tau)$) or less effective (decreased $V(\tau_)$ value).
%
In general, the metric should take the following conditions into account:
% 
\begin{enumerate}
    \item Correct accepted predictions should increase the value of $V(\tau)$, while incorrect accepted predictions should decrease the value of $V(\tau)$.
    \item Correct rejected predictions should decrease the value of $V(\tau)$, while incorrect rejected predictions should increase the value of $V(\tau)$.
\end{enumerate}
% 
We can convert these conditions into the following equations:
\begin{subequations}
    \label{for:conditions}
    \begin{align}
        \frac{\partial V}{\partial R_{tp}(\tau)} + \frac{\partial V}{\partial R_{tn}(\tau)} >= 0, &  &
        \frac{\partial V}{\partial R^r_{tp}(\tau)} + \frac{\partial V}{\partial R^r_{tn}(\tau)} <= 0, \label{for:conditions-tp-tn} \\
        \frac{\partial V}{\partial R_{fp}(\tau)} + \frac{\partial V}{\partial R_{fn}(\tau)} <= 0, &  &
        \frac{\partial V}{\partial R^r_{fp}(\tau)} + \frac{\partial V}{\partial R^r_{fn}(\tau)} >= 0, \label{for:conditions-fp-fn}
    \end{align}
\end{subequations}
%
where $R_t(\tau)$ and $R_t^r(\tau)$ are the fractions of accepted and rejected predictions respectively based on the rejection threshold $\tau$ and where $t \in [tp, tn, fp, fn]$.
%
All conditions include greater/smaller than or equal to operators, since we want to allow the values of $V_{tp}$, $V_{tn}$, $V_{fp}$, $V_{fn}$, and $V_{r}$ to be equal to zero.
%
We create a linear $V(\tau)$ function and assume that the values $V_t$ are known constants.
%
Subsequently, we can formulate $V(\tau)$ as:
\begin{align}
    \label{for:V}
    V(\tau) = \sum_{p} (V_p - V_r)R_p(\tau) + \sum_{q} (V_r - V_q)R^r_{q}(\tau),
\end{align}
%
where $p, q \in [tp, tn, fp, fn]$.
%
Conditions \ref{for:conditions-tp-tn} are satisfied by default since we assume that $V_{tp}$ and $V_{tn}$ are non-negative and $V_r$ is non-positive.
%
Conditions \ref{for:conditions-fp-fn} are satisfied under the condition of \ref{for:value-condition} and since we assume that $V_{fp}$, $V_{fn}$, and $V_r$ are negative.
% 
We can define the $R_t$ and the $R_t^r$ values by computing the integrals over the probability density functions (PDF) of the confidence values of the predictions with type $t$.
%
We can define $R_t$ by taking the integral over the interval $[\tau, 1]$, and $R_t^r$ by taking the integral over the interval $[0, \tau]$:
%
\begin{align}
    \label{for:R}
    \begin{aligned}
        R_{t}(\tau) = \int_\tau^1 D_{t}(x)dx & \quad & R_{t}^r(\tau) = \int_0^\tau D_{t}(x)dx,
    \end{aligned}
\end{align}
%
where $D_t$ is the PDF of all predictions of type $t$.
%
By inserting the integrals from \ref{for:R} into \ref{for:V}, we get our final value function:
%
\begin{align}
    \label{for:final-V}
    V(\tau) = \sum_{t} (V_t - V_r)\int_\tau^1 D_{t}(x)dx + \sum_{t} (V_r - V_t)\int_0^\tau D_{t}(x)dx
\end{align}
%
We can now use \ref{for:final-V} to calculate the total value of some ML model for all $\tau \in [0, 1]$ values.
%
The theoretical optimal rejection threshold is equal to the $\tau$ value for which we achieve the maximum value of $V(\tau)$.


\section{State-of-the-art}
\label{sec:state-of-the-art}
In this section, we will explain how we will apply the value-sensitive metric from section \ref{sec:value-metric} to some of the state-of-the-art automatic hate speech detection models so that we convert them into dependent rejectors.
%
In this experiment, we aim to find out three things.
%
First, we want to find out how the value-sensitive metric behaves on the different models and datasets.
%
Second, we want to know wheter rejecting ML predictions can be beneficial in the hate speech detection domain.
%
Finally, we will compare our value-sensitive metric to machine metrics such as accuracy and check whether they give different results.
%

\subsection{Models}
We will experiment with three different hate speech detection models based on the findings from related work in section \ref{sec:related-work-detection-algorithms}.
%
The first model is a traditional ML model.
%
We implement the Logistich Regression (LR) model with Character N-gram from \citet{waseem2016hateful} since this model achieved the best performance compared other traditional ML models \citet{davidson2017automated}.
%
The second model is DL model based on the work of \citet{agrawal2018deep} and \citet{badjatiya2017deep}.
%
We chose a Convolutional Neural Network (CNN) model since both studies found that it performs quite well.
%
Finally, our third model is a transformer model given its recent popularity in the NLP domain.
%
We selected the DistilBERT model since it's faster and smaller in size compared to BERT models while achieving similar performance \citep{sanh2019distilbert}.
%
We implement all models in Pyhthon.
%
We implement the LR model with scikit-learn\footnote{\url{https://scikit-learn.org/}}, the CNN model with TensorFlow\footnote{\url{https://www.tensorflow.org/}}, and the DistilBERT model with a combination of Hugging Face\footnote{\url{https://huggingface.co/}} and PyTorch\footnote{\url{https://pytorch.org/}}.

\subsection{Calibration}
The problem of most neural network models is that they are often not calibrated \citep{guo2017calibration,sayin2021science}.
%
We can define calibrated models as models where the confidence values of the predictions are equal to the probabilities that the predicted labels are correct.
%
However, most neural networks tend to be sensitive to producing both low- and high-confident errors \citep{guo2017calibration, sayin2021science}.
%
A well-calibrated model that achieves a low accuracy score can still be valuable since we can reject all low-confident incorrect predictions and only accept the high-confident correct predictions \citep{sayin2021science}.
%
In our project, we aim to have calibrated models since calculating the optimal rejection threshold depends on the confidence values of the predictions.
%

%
\citet{guo2017calibration} experimented with different calibration methods.
%
They evaluated the results using the expected calibration error (ECE), which measures the difference between the expected confidence and accuracy \citep{guo2017calibration}.
%
They found that the temperature scaling method is the most effective.
%
In temperature scaling, we divide the model's output logits with a temperature value $T$ to soften the probabilities of the final softmax function in the model's architecture \citep{guo2017calibration}.
%
This $T$ value is initially set to $1$ and optimized by minimizing the negative log-likelihood \citep{guo2017calibration}.
%
Please note that temperature scaling does not change the model's accuracy but only rescales the distribution of the confidence values \citep{guo2017calibration}.
%

%
As we will experiment with two neural networks (DistilBERT and CNN), we will apply temperature scaling to calibrate both models.
%
Unfortunately, it is also possible that once we calibrated these models with temperature scaling, they can still produce high-confident incorrect predictions and low-confident correct predictions.
%
However, it is still valuable to calibrate the models since it also benefits human interpretation of the confidence values and, therefore, the optimal rejection threshold.
%

\subsection{Datasets}
We train all models on the \citet{waseem2016hateful} dataset consisting of 16K tweets labelled racist, sexist, or neutral.
%
We converted the 'racist' and 'sexist' labels to 'hate' labels to create a binary classification setting.
%
We split the dataset into a train and test dataset according to an 80:20 ratio, respectively (80\% for training and 20\% for testing)
%
For the CNN and the DistilBERT models, we split the training set up into a training set and a validation set according to a 75:25 ratio, respectively.
%
We use this validation set to calibrate the trained models by finding the optimal temperature $T$ value for the temperature scaling method.
%
We preprocess the data by tokenizing all URLs, user mentions, and emojis since these do not contain any valuable information.
% TODO: perhaps add wordsegment library for segmenting the hashtags.
The remaining parts of the preprocessing, such as removing whitespaces and stop words or the tokenization process, are dedicated to the different frameworks we use per model.
%

%
We apply the metric to two test datasets: the \emph{seen} dataset and the \textit{unseen} dataset.
%
The seen dataset is the test set from the \citet{waseem2016hateful} dataset.
%
The unseen dataset is a test set from the \citet{basile2019semeval} dataset that consists of 10K English tweets labelled as either hateful (against immigrants or women) or not hateful.
%
We use the unseen dataset to simulate how the models would perform in a realist use-case when a model is trained on one dataset and applied to a different dataset.
%
We want to analyze the effect of bias and how this affects the results when using our metric for evaluating the models when rejection is adopted.
%
We expect that the accuracy of the predictions on the unseen dataset is significantly lower than on the seen dataset, similar to the findings of related studies: \citet{grondahl2018all, arango2019hate}.
%
Therefore, we also expect that the output value of our metric for the unseen dataset will be lower and that the optimal rejection threshold will be lower (meaning that we need to reject more predictions).
%


\subsection{Calculating the total value}
\todo[inline]{Explain how we are going to apply our metric to the trained state-of-the-art models to calculate the optimal rejection threshold given the values of TP, TN, FP, FN, and rejection (that we still need estimate)}
\todo[inline]{Explain that we use Kernel Density Estimation to calculate the PDF functions (or stick to prediction counts as in the paper).}
We apply our metric from section \ref{sec:value-metric} to calculate the total value of all three models for both the seen and the unseen datasets at all possible rejection thresholds ($\tau$).
%
Since our metric uses the PDFs of all TP, TN, FP, and FN predictions, we need to emperically estimate these PDFs since we do not know the actual underlying distributions.
%
We use Kernel Density Estimation (KDE) for this.....
% TODO: continue here
%
If $\tau \in [0.0, 0.5]$, then we accept all predictions since the confidence values of all predictions are always above $0.5$ given our binary clasfficastion setting.
%
If $\tau = 1.0$, then we reject all predictions.
