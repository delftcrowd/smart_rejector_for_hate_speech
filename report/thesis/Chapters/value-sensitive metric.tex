\chapter{Value-sensitive rejection}
As concluded in the \nameref{sec:related-work}, there is a need for \textit{value-sensitive} metrics for measuring the performance of ML models, especially for social-technical applications such as hate speech detection.
%
We also concluded that manual human moderation is the most effective and that most automatic hate speech detection methods do not perform well on unseen data.
%
Therefore, in this project, we focus on rejecting ML predictions in a value-sensitive manner.
%
We do this by taking the values of TP, TN, FP, FN, and rejected predictions into account.
%
Correct predictions (TP and TN) result in positive value (gains), while incorrect (FP and FN) and rejected predictions result in negative value (costs).
%
In this section, we assume that we know these values.
%
However, section \ref{sec:survey} will explain how we assess these values.
%

%
In this project, we create a dependent rejector by introducing a confidence metric that measures the total value of an ML model for some rejection threshold value based on a set of predictions with their corresponding confidence values and the values of TP, TN, FP, FN, and rejected predictions.
%
The idea of rejecting ML predictions with a confidence metric is that we aim to find the optimal rejection threshold between 0 (accepting all predictions) and 1 (rejecting all predictions).
%
Suppose the optimal confidence rejection threshold is 0.5, then we accept all predictions with a confidence value greater than or equal to 0.5 and reject all predictions with a confidence value below 0.5.
%
We can determine the optimal rejection threshold by finding the threshold value for which the total value of the ML model with rejection is maximum.
%

%
In \ref{sec:value-metric}, we explain how we construct our confidence metric, and in \ref{sec:state-of-the-art}, we discuss how we apply the metric to some of the state-of-the-art hate speech classification models.

\section{Value metric}
\label{sec:value-metric}
\todo[inline]{Explain and proof our value metric and how we use it to measure the total value of a ML model with a reject option}

\section{State-of-the-art}
\label{sec:state-of-the-art}
\subsection{Models}
\todo[inline]{Explain that we are going to use one traditional ML model (Logistic Regression since in related work we found that this got the best performance), one DL model (CNN because of the same reason), and a transformer model (DistilBERT given the recent popularity of transformer models)}

\subsection{Calibration}
\todo[inline]{Explain what model calibration is and why it's necessary}

\subsection{Datasets}
\todo[inline]{Explain that we are going to use the SemEval and Waseem datasets and the concepts of seen and unseen data}

\subsection{Calculating the total value}
\todo[inline]{Explain how we are going to apply our metric to the trained state-of-the-art models to calculate the optimal rejection threshold given the values of TP, TN, FP, FN, and rejection (that we still need estimate)}
\todo[inline]{Explain that we use Kernel Density Estimation to calculate the PDF functions (or stick to prediction counts as in the paper).}
