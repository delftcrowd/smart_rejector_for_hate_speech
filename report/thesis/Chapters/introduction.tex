\chapter{Introduction}
\label{ch:introduction}
\newcommand{\customtextbox}[1]{
	\setlength{\fboxsep}{0.5em}
	\fbox{
		\begin{minipage}{\linewidth-1.7em}
			\vspace*{0.25em}
			#1
		\end{minipage}
	}
}

The amount of hateful content spread online on social media platforms remains a significant problem. Ignoring its presence can harm people and even result in actual violence and other conflicts \citep{ecri-hate-speech-and-violence, balayn2021automatic}. There are many news articles about events where hate spread on online platforms lead to acts of violence \citep{columbia-facebook-linked-to-violence, mujib-mashal-india, paul-mozur-2018, muller2021fanning}. One research paper found a connection between hateful content on Facebook containing anti-refugee sentiment and hate crimes against refugees by analyzing social media usage in multiple municipalities in Germany \citep{muller2021fanning}. Governmental institutions and social media companies are becoming more aware of these risks and are trying to combat hate speech. For example, the European Union developed a Code of Conduct on countering illegal hate speech in cooperation with large social media companies such as Facebook and Twitter \citep{eu-code-of-conduct}. This Code of Conduct requests companies to prohibit hate speech and report their progress every year \citep{eu-code-of-conduct}. The most recent report from 2021 stated that Twitter only removed 49.5\% of all hateful content on their platform. Facebook is most successful in removing hate speech as they claim to have removed 70.2\% of all hateful content in 2021 \citep{eu-code-of-conduct}. However, one article found in internal communication from Facebook that this percentage is much lower, around 3-5\% \citep{noah2021giansiracusa}. Therefore, hate speech detection remains a hard problem that even large institutions have not solved yet.

Currently, people rely on reactive and proactive content moderation methods to detect hate speech \citep{klonick2017new}. Reactive moderation is when social media users are flagging (also known as reporting) hateful content \citep{klonick2017new}. Proactive moderation is either done automatically using detection algorithms or manually by a group of human moderators \citep{klonick2017new}. There exist different methods for automatically detecting hateful content. Most use Machine Learning (ML) algorithms since these tend to be the most promising for their detection performance at a large scale \citep{balayn2021automatic, fortuna2018survey}. These algorithms can range from traditional ML methods such as Support Vector Machine or Decision Tree to Deep Learning algorithms \citep{fortuna2018survey}.

However, both proactive and reactive moderation methods have their limitations. Proactive manual moderation of hateful content is still the most reliable solution but is simply infeasible due to the large amount of content generated by the many users \citep{balayn2021automatic}. Reactive moderation solves this problem since the users can report hate speech themselves. Although, the problem stays that hateful content is exposed to the users for some time. Proactive automatic moderation using automated detection algorithms allow for large amounts of data to be checked quickly without the involvement of humans. However, these algorithms have shown to be unreliable as they often perform poor on deployment data \citep{balayn2021automatic, grondahl2018all}. One study found that the F1 scores reduce significantly (69\% F1 score drop in the worst case) when training a hate speech detection model on one dataset and evaluating it using another dataset \citep{grondahl2018all}. Furthermore, one paper found that most research in hate speech detection overestimates the performance of the automated detection methods \citep{arango2019hate}. The authors found that the performance drops significantly when the detection algorithms are trained on one dataset and evaluated on another \citep{arango2019hate}.

This thesis research will tackle the problems of proactive moderation by focusing on the concept of \textit{human-machine co-creation} \citep{woo2020future} where the advantages of both humans (cognitive abilities and ability to make judgements) and machines (automation and performance) are combined. So humans and machines should work together to detect hate speech. ML models should detect hateful content automatically and humans should make the final decisions (\textit{human-in-the-loop}) when the model is not confident enough \citep{woo2020future}. Here come ML models with a reject option in place. The goal of the reject option is to reject an ML prediction when the risk of making an incorrect prediction is too high and to defer the prediction task to a human \citep{hendrickx2021machine}. There are several advantages. First, the utility of the ML model increases as only the most confident (and possibly the most correct) predictions are accepted. Second, less human effort is necessary as the machine is handling all prediction tasks, and only a fraction needs to be checked by a human. To the best of our knowledge, ML with rejection has not been used in hate speech detection before.

In this work, we focus on \textit{value-senstive} rejection. There are gains of accepting correct predictions (positive value) and costs of accepting incorrect or rejecting predictions (negative value). We should weigh these values according to the task of hate speech detection and incorporate them in the design of the hybrid human-AI system \citep{sayin2021science}. We will mainly focus on the user-centred value since the social media users are the most affected by the consequences of hate speech.

The idea of most ML models with rejection is that we reject predictions when the model's confidence is too low. Therefore, we need a metric that measures the total value of ML models with a reject option. We can use the resulting metric to determine when to reject/accept predictions by maximizing the total value. Second, we need to find out how we can define the user-centred values in the context of hate speech detection. We will attempt to retrieve the value ratios since it is hard to come up with the absolute cost values in the hate speech domain. By value ratios, we mean to figure out, for example, the ratio between an FP and an FN prediction. Therefore, our first sub-research question is as follows:

This leads to the following research questions:

% \noindent\customtextbox{\textbf{RQ} How can we maximize the value of Machine Learning models in hate speech detection using a reject option?}
% \noindent\customtextbox{\textbf{RQ} How can we reject predictions of Machine Learning models in hate speech detection in a value-sensitive manner?}
% \textbf{RQ} How can we maximize the value of Machine Learning models in hate speech detection using a reject option?

\noindent\customtextbox{
	\textbf{RQ} How can we reject predictions of Machine Learning models in a value-sensitive manner for hate speech detection ?
	\begin{itemize}
		\item \textbf{SRQ1} How can we measure the total value of Machine Learning models with a reject option?
		\item \textbf{SRQ2} How can we determine the value ratios between rejections and True Positive (TP), True Negative (TN), False Positive (FP), and False Negative (FN) predictions?
	\end{itemize}
}

% The second problem is about detecting the low and high confidence errors. These errors are also called \textit{unknown (un)knowns} \citep{liu2020towards}. When we would only rely on the confidence of the ML model to determine when to reject/accept predictions, then we would accept a subset of incorrect predictions with high confidence, and we would reject a subset of correct predictions with low confidence. We need to find a way to recognize these unknown (un)knowns. Once detected, we can reject the unknown unknowns so that a human moderator makes the final judgement and accept the unknown knowns to save the human moderator extra work. Doing so would further improve the utility of our smart rejector for detecting hate speech. So, our second sub research question is as follows:

% \todo[inline]{I will address the second sub research question only if there is enough time left.}

% \customtextbox{\textbf{SRQ2} How can we detect the unknown (un)knowns?}

% Finally, we need to find out how we can combine our findings into one smart rejection system which leads to our final sub research question:

% \customtextbox{\textbf{SRQ3} How can we build the smart rejector?}

\todo[inline]{Here comes a list of contributions}

\todo[inline]{Here comes a short description of the structure of the thesis report}
